{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 AI Generator Agent Tutorial (Advanced): Advanced Production Features\n",
    "\n",
    "Welcome to **Part 3** of the AI Generator Agent tutorial! This builds on the concepts from Part 2A and demonstrates the advanced, production-ready features used in the Orion AI Agent System.\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "- **LangChain Integration**: Professional AI workflow orchestration\n",
    "- **GPT Optimization**: Advanced model configuration and validation\n",
    "- **Repository-Aware Context**: Intelligent codebase analysis\n",
    "- **Code Modification**: Precise file editing with validation\n",
    "- **State Management**: Persistent tracking and analytics\n",
    "- **Multi-Agent Context Passing**: Integration with other AI agents\n",
    "\n",
    "## 🔄 Prerequisites\n",
    "\n",
    "- Completed Part 2 (Core Concepts Tutorial)\n",
    "- OpenRouter API key (for GPT access)\n",
    "- Understanding of LangChain concepts\n",
    "- Familiarity with Pydantic models\n",
    "\n",
    "## 🏗️ Architecture Overview\n",
    "\n",
    "This tutorial demonstrates the production architecture used in Orion:\n",
    "\n",
    "```\n",
    "LangChain Orchestration\n",
    "    ↓\n",
    "GPT Model (Optimized)\n",
    "    ↓\n",
    "Repository Context Analysis\n",
    "    ↓\n",
    "Structured Output (Pydantic)\n",
    "    ↓\n",
    "File Operations (Create/Modify)\n",
    "    ↓\n",
    "State Management & Tracking\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔑 OpenRouter Configuration for Advanced Models\n",
    "\n",
    "Let's set up OpenRouter with advanced model validation and optimization for code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for OpenRouter API key (same as Part 2A)\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Enhanced Pydantic Models for Production\n",
    "\n",
    "Let's create the advanced Pydantic models used in the production system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Pydantic model matching the production system\n",
    "class CodeGenerationResponse(BaseModel):\n",
    "    \"\"\"Enhanced structured response for production code generation.\"\"\"\n",
    "\n",
    "    success: bool = Field(description=\"Whether the generation was successful\")\n",
    "    files: List[Dict[str, str]] = Field(\n",
    "        description=\"Generated files with name and content\"\n",
    "    )\n",
    "    modifications: List[Dict[str, str]] = Field(\n",
    "        description=\"File modifications with target and changes\", default=[]\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step explanation of the approach and decisions\"\n",
    "    )\n",
    "    dependencies: List[str] = Field(description=\"Required dependencies\", default=[])\n",
    "    next_steps: List[str] = Field(description=\"Recommended next steps\", default=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Agent State Management (simplified version)\n",
    "class BaseAgentState:\n",
    "    \"\"\"Simple state management for our advanced AI generator.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_name: str):\n",
    "        self.agent_name = agent_name\n",
    "        self.state = {}\n",
    "        self.debug = False\n",
    "\n",
    "    def update_state(self, key: str, value: Any) -> None:\n",
    "        \"\"\"Update state value.\"\"\"\n",
    "        self.state[key] = value\n",
    "\n",
    "    def get_state(self, key: str, default: Any = None) -> Any:\n",
    "        \"\"\"Get state value with default.\"\"\"\n",
    "        return self.state.get(key, default)\n",
    "\n",
    "    def log(self, message: str, level: str = \"info\") -> None:\n",
    "        \"\"\"Simple logging function.\"\"\"\n",
    "        timestamp = time.strftime(\"%H:%M:%S\")\n",
    "        level_emoji = {\"info\": \"ℹ️\", \"error\": \"❌\", \"warning\": \"⚠️\", \"success\": \"✅\"}\n",
    "        emoji = level_emoji.get(level, \"ℹ️\")\n",
    "        print(f\"[{timestamp}] {emoji} [{self.agent_name}] {message}\")\n",
    "\n",
    "    def execute_with_tracking(self, operation_name: str, operation_func):\n",
    "        \"\"\"Execute operation with tracking.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.log(f\"Starting operation: {operation_name}\")\n",
    "\n",
    "        result = operation_func()\n",
    "        duration = time.time() - start_time\n",
    "        self.log(\n",
    "            f\"Operation {operation_name} completed in {duration:.2f}s\", \"success\"\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Advanced AI Generator with LangChain Integration\n",
    "\n",
    "Now let's build the production-ready AI Generator with LangChain orchestration and GPT-5 optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Advanced AI Generator Agent class structure defined!\n"
     ]
    }
   ],
   "source": [
    "class AdvancedAIGeneratorAgent(BaseAgentState):\n",
    "    \"\"\"\n",
    "    Production-ready AI Generator Agent with LangChain and OpenRouter.\n",
    "\n",
    "    Features:\n",
    "    - Advanced model support through OpenRouter\n",
    "    - LangChain orchestration for complex workflows\n",
    "    - Repository-aware context building\n",
    "    - Code modification capabilities\n",
    "    - Production error handling and fallbacks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"openai/gpt-4o-mini\",\n",
    "        temperature: float = 0.1,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        \"\"\"Initialize the Advanced AI Generator Agent.\"\"\"\n",
    "        super().__init__(\"AdvancedAIGenerator\")\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.debug = debug\n",
    "\n",
    "        # Initialize LangChain components with OpenRouter (without JSON response format)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=32768,  # Advanced models support large outputs\n",
    "            api_key=openrouter_api_key,\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "        )\n",
    "\n",
    "        self.log(f\"🤖 Initialized LangChain with OpenRouter model: {model}\")\n",
    "\n",
    "        # Initialize state\n",
    "        self.update_state(\"model\", model)\n",
    "        self.update_state(\"temperature\", temperature)\n",
    "        self.update_state(\"generated_code\", [])\n",
    "        self.update_state(\"created_files\", [])\n",
    "        self.update_state(\"modified_files\", [])\n",
    "\n",
    "    def build_repository_context(self, repo_path: str) -> str:\n",
    "        \"\"\"Build intelligent repository context by analyzing existing code.\"\"\"\n",
    "        if not os.path.exists(repo_path):\n",
    "            return \"Repository path does not exist\"\n",
    "\n",
    "        context_parts = []\n",
    "        supported_extensions = (\n",
    "            \".py\",\n",
    "            \".js\",\n",
    "            \".ts\",\n",
    "            \".java\",\n",
    "            \".go\",\n",
    "            \".rs\",\n",
    "            \".cpp\",\n",
    "            \".c\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            files_found = []\n",
    "            total_lines = 0\n",
    "\n",
    "            # Walk through repository and analyze files\n",
    "            for root, _, filenames in os.walk(repo_path):\n",
    "                for filename in filenames[:15]:  # Limit to first 15 files for context\n",
    "                    if filename.endswith(supported_extensions):\n",
    "                        rel_path = os.path.relpath(\n",
    "                            os.path.join(root, filename), repo_path\n",
    "                        )\n",
    "                        files_found.append(rel_path)\n",
    "\n",
    "                        # Analyze file content for additional context\n",
    "                        try:\n",
    "                            full_path = os.path.join(root, filename)\n",
    "                            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                                content = f.read()\n",
    "                                total_lines += len(content.split(\"\\n\"))\n",
    "                        except:\n",
    "                            pass  # Skip files that can't be read\n",
    "\n",
    "            if files_found:\n",
    "                context_parts.append(\n",
    "                    f\"- Repository contains {len(files_found)} code files ({total_lines} total lines)\"\n",
    "                )\n",
    "                context_parts.append(f\"- Key files: {', '.join(files_found[:10])}\")\n",
    "\n",
    "                # Detect project type based on files\n",
    "                if any(f.endswith(\".py\") for f in files_found):\n",
    "                    context_parts.append(\"- Project type: Python\")\n",
    "                if any(f.endswith((\".js\", \".ts\")) for f in files_found):\n",
    "                    context_parts.append(\"- Project type: JavaScript/TypeScript\")\n",
    "                if any(f.endswith(\".java\") for f in files_found):\n",
    "                    context_parts.append(\"- Project type: Java\")\n",
    "\n",
    "                # Check for common framework files\n",
    "                framework_indicators = {\n",
    "                    \"requirements.txt\": \"Python project with pip dependencies\",\n",
    "                    \"package.json\": \"Node.js project with npm dependencies\",\n",
    "                    \"Cargo.toml\": \"Rust project with cargo dependencies\",\n",
    "                    \"pom.xml\": \"Java project with Maven dependencies\",\n",
    "                    \"build.gradle\": \"Java project with Gradle dependencies\",\n",
    "                }\n",
    "\n",
    "                for indicator, description in framework_indicators.items():\n",
    "                    if os.path.exists(os.path.join(repo_path, indicator)):\n",
    "                        context_parts.append(f\"- {description}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            context_parts.append(f\"- Repository analysis failed: {str(e)}\")\n",
    "\n",
    "        return (\n",
    "            \"\\n\".join(context_parts)\n",
    "            if context_parts\n",
    "            else \"No code files found in repository\"\n",
    "        )\n",
    "\n",
    "    def build_enhanced_context(self, context: Optional[Dict], repo_path: str) -> str:\n",
    "        \"\"\"Build enhanced context combining user context and repository analysis.\"\"\"\n",
    "        context_parts = []\n",
    "\n",
    "        # Add user-provided context\n",
    "        if context:\n",
    "            context_parts.append(\"USER CONTEXT:\")\n",
    "            for key, value in context.items():\n",
    "                if isinstance(value, list):\n",
    "                    value = \", \".join(str(v) for v in value)\n",
    "                context_parts.append(f\"- {key}: {value}\")\n",
    "\n",
    "        # Add repository context\n",
    "        repo_context = self.build_repository_context(repo_path)\n",
    "        if repo_context:\n",
    "            context_parts.append(\"\\nREPOSITORY CONTEXT:\")\n",
    "            context_parts.append(repo_context)\n",
    "\n",
    "        return (\n",
    "            \"\\n\".join(context_parts)\n",
    "            if context_parts\n",
    "            else \"No additional context provided\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"🏗️ Advanced AI Generator Agent class structure defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_changes(self, prompt: str, repo_path: str, context: Optional[Dict] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Generate code changes using advanced models through OpenRouter with LangChain orchestration.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Task description for the AI\n",
    "        repo_path: Path to the repository\n",
    "        context: Additional context for generation\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated code changes with reasoning, or None if failed\n",
    "    \"\"\"\n",
    "\n",
    "    def _generation_operation():\n",
    "        # Enhanced prompt template for advanced models with reasoning\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert software architect using advanced AI capabilities.\n",
    "\n",
    "CORE COMPETENCIES:\n",
    "- Software architecture and design patterns\n",
    "- Modern development practices and frameworks\n",
    "- Code quality, security, and performance optimization\n",
    "- Cross-platform and cloud-native development\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "You MUST respond with valid JSON in this exact structure:\n",
    "{{\n",
    "  \"success\": true,\n",
    "  \"files\": [\n",
    "    {{\"name\": \"filename.py\", \"content\": \"complete production-ready code\"}},\n",
    "    {{\"name\": \"requirements.txt\", \"content\": \"package==version\\\\npackage2==version\"}}\n",
    "  ],\n",
    "  \"modifications\": [\n",
    "    {{\"target\": \"existing_file.py\", \"changes\": \"specific changes to make\"}}\n",
    "  ],\n",
    "  \"reasoning\": \"Detailed step-by-step analysis of the problem, solution approach, design decisions, and trade-offs considered\",\n",
    "  \"dependencies\": [\"package1\", \"package2\"],\n",
    "  \"next_steps\": [\"step 1\", \"step 2\"]\n",
    "}}\n",
    "\n",
    "REASONING PROCESS:\n",
    "1. ANALYZE: Break down requirements and constraints\n",
    "2. DESIGN: Plan architecture and component interactions\n",
    "3. IMPLEMENT: Generate production-ready code with best practices\n",
    "4. VALIDATE: Consider edge cases, security, and performance\n",
    "\n",
    "QUALITY STANDARDS:\n",
    "- Complete, runnable code with all imports\n",
    "- Comprehensive error handling and logging\n",
    "- Type hints and detailed Google-style docstrings\n",
    "- Security best practices and input validation\n",
    "- Performance optimization and resource management\n",
    "\n",
    "Please provide your response in JSON format as specified above.\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"DEVELOPMENT TASK:\n",
    "Repository: {repo_path}\n",
    "Task: {prompt}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "REQUIREMENTS:\n",
    "Generate a complete, production-ready solution that follows best practices.\n",
    "Include detailed reasoning about your approach and design decisions.\n",
    "Consider scalability, maintainability, and security in your solution.\n",
    "Respond with valid JSON as specified in the system message.\"\"\"\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Use string output parser and handle JSON manually\n",
    "        chain = template | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Build enhanced context\n",
    "        context_str = self.build_enhanced_context(context, repo_path)\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\n",
    "                \"prompt\": prompt,\n",
    "                \"repo_path\": repo_path,\n",
    "                \"context\": context_str\n",
    "            })\n",
    "            \n",
    "            # Try to parse JSON from response\n",
    "            try:\n",
    "                # Clean the response - remove markdown code blocks if present\n",
    "                json_str = response.strip()\n",
    "                if json_str.startswith('```json'):\n",
    "                    json_str = json_str[7:]  # Remove ```json\n",
    "                if json_str.startswith('```'):\n",
    "                    json_str = json_str[3:]   # Remove ```\n",
    "                if json_str.endswith('```'):\n",
    "                    json_str = json_str[:-3]  # Remove closing ```\n",
    "                    \n",
    "                result = json.loads(json_str)\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                self.log(\"❌ Failed to parse JSON response, using fallback\", \"warning\")\n",
    "                return self._fallback_generation(prompt, repo_path, context_str)\n",
    "            \n",
    "            # Format result for output\n",
    "            if isinstance(result, dict):\n",
    "                formatted_result = self._format_structured_result(result)\n",
    "            else:\n",
    "                formatted_result = str(result)\n",
    "            \n",
    "            # Update state with generation info\n",
    "            generation_info = {\n",
    "                \"prompt\": prompt,\n",
    "                \"repo_path\": repo_path,\n",
    "                \"context\": context,\n",
    "                \"result\": result,\n",
    "                \"formatted_result\": formatted_result,\n",
    "                \"timestamp\": time.time(),\n",
    "                \"model\": self.model,\n",
    "                \"success\": result.get(\"success\", False) if isinstance(result, dict) else True,\n",
    "                \"reasoning\": result.get(\"reasoning\", \"\") if isinstance(result, dict) else \"\",\n",
    "            }\n",
    "            \n",
    "            generated_code = self.get_state(\"generated_code\", [])\n",
    "            generated_code.append(generation_info)\n",
    "            self.update_state(\"generated_code\", generated_code)\n",
    "            \n",
    "            return formatted_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"❌ Advanced model generation failed: {e}\", \"error\")\n",
    "            return self._fallback_generation(prompt, repo_path, context_str)\n",
    "    \n",
    "    return self.execute_with_tracking(\"generate_code_changes\", _generation_operation)\n",
    "\n",
    "AdvancedAIGeneratorAgent.generate_code_changes = generate_code_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_structured_result(self, result: Dict) -> str:\n",
    "    \"\"\"Format structured result back to expected string format.\"\"\"\n",
    "    formatted_parts = []\n",
    "    \n",
    "    # Add reasoning as comment\n",
    "    if result.get(\"reasoning\"):\n",
    "        reasoning_lines = result[\"reasoning\"].split(\"\\n\")\n",
    "        formatted_reasoning = \"\\n# \".join([\"REASONING:\"] + reasoning_lines)\n",
    "        formatted_parts.append(f\"# {formatted_reasoning}\\n\")\n",
    "    \n",
    "    # Format file creations\n",
    "    for file_info in result.get(\"files\", []):\n",
    "        name = file_info.get(\"name\", \"unknown.py\")\n",
    "        content = file_info.get(\"content\", \"\")\n",
    "        formatted_parts.append(f\"FILE: {name}\\n```python\\n{content}\\n```\\n\")\n",
    "    \n",
    "    # Format modifications\n",
    "    for mod_info in result.get(\"modifications\", []):\n",
    "        target = mod_info.get(\"target\", \"unknown.py\")\n",
    "        changes = mod_info.get(\"changes\", \"\")\n",
    "        formatted_parts.append(f\"MODIFY: {target}\\n```\\n{changes}\\n```\\n\")\n",
    "    \n",
    "    # Add dependencies and next steps as comments\n",
    "    if result.get(\"dependencies\"):\n",
    "        deps = \", \".join(result[\"dependencies\"])\n",
    "        formatted_parts.append(f\"# DEPENDENCIES: {deps}\\n\")\n",
    "    \n",
    "    if result.get(\"next_steps\"):\n",
    "        steps = \"\\n# \".join(result[\"next_steps\"])\n",
    "        formatted_parts.append(f\"# NEXT STEPS:\\n# {steps}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_parts)\n",
    "\n",
    "AdvancedAIGeneratorAgent._format_structured_result = _format_structured_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fallback_generation(self, prompt: str, repo_path: str, context_str: str) -> str:\n",
    "    \"\"\"Fallback generation method using simpler approach.\"\"\"\n",
    "    self.log(\"🔄 Using fallback generation method...\", \"warning\")\n",
    "    \n",
    "    fallback_template = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert Python developer. Generate complete, production-ready code files.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "For each file, use this EXACT format:\n",
    "\n",
    "FILE: filename.py\n",
    "```python\n",
    "# Complete file content with imports, classes, functions\n",
    "# Include proper docstrings and type hints\n",
    "```\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Complete, runnable code\n",
    "- All necessary imports\n",
    "- Google-style docstrings\n",
    "- Type hints\n",
    "- Error handling\n",
    "- Follow PEP 8\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Repository: {repo_path}\\nTask: {prompt}\\nContext: {context}\"\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    chain = fallback_template | self.llm | StrOutputParser()\n",
    "    return chain.invoke({\n",
    "        \"prompt\": prompt,\n",
    "        \"repo_path\": repo_path,\n",
    "        \"context\": context_str\n",
    "    })\n",
    "\n",
    "AdvancedAIGeneratorAgent._fallback_generation = _fallback_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the change isn't too drastic\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def modify_existing_file(\n",
    "    self,\n",
    "    repo_path: str,\n",
    "    file_path: str,\n",
    "    modification_description: str,\n",
    "    file_content: Optional[str] = None,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Modify an existing file with precision using advanced models.\n",
    "\n",
    "    Args:\n",
    "        repo_path: Repository path\n",
    "        file_path: Path to file to modify\n",
    "        modification_description: Description of changes to make\n",
    "        file_content: Optional file content (will read from disk if not provided)\n",
    "\n",
    "    Returns:\n",
    "        str: Modified file content or None if failed\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        self.log(f\"🔧 Modifying file with advanced model: {file_path}\")\n",
    "\n",
    "        # Construct full file path\n",
    "        if os.path.isabs(file_path):\n",
    "            full_file_path = file_path\n",
    "        else:\n",
    "            full_file_path = os.path.join(repo_path, file_path)\n",
    "\n",
    "        # Read existing content\n",
    "        if file_content is None:\n",
    "            if not os.path.exists(full_file_path):\n",
    "                self.log(f\"❌ File not found: {full_file_path}\", \"error\")\n",
    "                return None\n",
    "\n",
    "            with open(full_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_content = f.read()\n",
    "        else:\n",
    "            existing_content = file_content\n",
    "\n",
    "        self.log(f\"📄 Original file content ({len(existing_content)} chars)\")\n",
    "\n",
    "        # Use advanced model for precise modification with precision prompts\n",
    "        modification_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"You are a precise code editor using advanced AI capabilities.\n",
    "\n",
    "TASK: Modify code files with precision while preserving all existing functionality.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- Make ONLY the requested change - do not refactor, reorganize, or \"improve\" other code\n",
    "- Preserve ALL existing imports, comments, formatting, and structure\n",
    "- Maintain the exact same file structure and organization\n",
    "- Keep all existing functionality intact\n",
    "- If changing a value, change ONLY that specific value\n",
    "- If modifying a function, change ONLY what was requested\n",
    "\n",
    "OUTPUT: Return the complete modified file content with only the minimal necessary change applied.\"\"\",\n",
    "                ),\n",
    "                (\n",
    "                    \"user\",\n",
    "                    \"\"\"MODIFICATION REQUEST: {modification_description}\n",
    "\n",
    "ORIGINAL FILE CONTENT:\n",
    "```\n",
    "{existing_content}\n",
    "```\n",
    "\n",
    "Apply the requested change with precision, keeping everything else exactly the same:\"\"\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        modification_chain = modification_template | self.llm | StrOutputParser()\n",
    "        modified_content = modification_chain.invoke(\n",
    "            {\n",
    "                \"existing_content\": existing_content,\n",
    "                \"modification_description\": modification_description,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Clean up the response (remove code block markers if present)\n",
    "        modified_content = modified_content.strip()\n",
    "        if modified_content.startswith(\"```\"):\n",
    "            lines = modified_content.split(\"\\n\")\n",
    "            if len(lines) > 2:\n",
    "                modified_content = \"\\n\".join(lines[1:-1])\n",
    "\n",
    "        # Verify that the change isn't too drastic using similarity check\n",
    "        similarity = SequenceMatcher(None, existing_content, modified_content).ratio()\n",
    "\n",
    "        if similarity < 0.5:  # If less than 50% similar, it's probably wrong\n",
    "            self.log(\n",
    "                f\"❌ Modification too drastic (similarity: {similarity:.2f}), rejecting\",\n",
    "                \"error\",\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        self.log(\n",
    "            f\"✅ Advanced model modification applied (similarity: {similarity:.2f})\"\n",
    "        )\n",
    "\n",
    "        # Write the modified content and track it\n",
    "        self._write_modified_file(full_file_path, existing_content, modified_content)\n",
    "        return modified_content\n",
    "\n",
    "    except Exception as e:\n",
    "        self.log(f\"❌ Advanced model modification failed: {e}\", \"error\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _write_modified_file(\n",
    "    self, full_file_path: str, original_content: str, modified_content: str\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Write modified content to a file and track the modification.\n",
    "\n",
    "    Args:\n",
    "        full_file_path: Full path to the file\n",
    "        original_content: Original file content\n",
    "        modified_content: Modified file content\n",
    "\n",
    "    Returns:\n",
    "        bool: True if successful\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(full_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(modified_content)\n",
    "\n",
    "        self.log(f\"✅ Modified file: {full_file_path}\")\n",
    "\n",
    "        # Track modified file\n",
    "        modified_files = self.get_state(\"modified_files\", [])\n",
    "        modified_files.append(\n",
    "            {\n",
    "                \"path\": full_file_path,\n",
    "                \"original_size\": len(original_content),\n",
    "                \"modified_size\": len(modified_content),\n",
    "                \"timestamp\": time.time(),\n",
    "                \"similarity\": SequenceMatcher(\n",
    "                    None, original_content, modified_content\n",
    "                ).ratio(),\n",
    "            }\n",
    "        )\n",
    "        self.update_state(\"modified_files\", modified_files)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        self.log(f\"❌ Failed to write modified file {full_file_path}: {e}\", \"error\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Add modification methods to the class\n",
    "AdvancedAIGeneratorAgent.modify_existing_file = modify_existing_file\n",
    "AdvancedAIGeneratorAgent._write_modified_file = _write_modified_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(self) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get comprehensive information about the current advanced model.\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Model information and capabilities\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": self.model,\n",
    "        \"temperature\": self.temperature,\n",
    "        \"max_tokens\": 32768,\n",
    "        \"supports_structured_output\": True,\n",
    "        \"supports_reasoning\": True,\n",
    "        \"context_window\": 128000,  # Most advanced models support large context\n",
    "        \"provider\": \"OpenRouter\",\n",
    "        \"langchain_integration\": self.llm is not None,\n",
    "        \"state_management\": True,\n",
    "    }\n",
    "\n",
    "def make_code_changes(self, generated_code: str, repo_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Apply generated code changes to the repository.\n",
    "    \n",
    "    Args:\n",
    "        generated_code: The generated code with FILE: and MODIFY: directives\n",
    "        repo_path: Path to the repository\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        self.log(\"📝 Applying generated code changes to repository\")\n",
    "        \n",
    "        # Parse the generated code for FILE: and MODIFY: directives\n",
    "        lines = generated_code.split('\\n')\n",
    "        current_file = None\n",
    "        current_content = []\n",
    "        files_created = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            \n",
    "            # Check for FILE: directive\n",
    "            if line.strip().startswith('FILE:'):\n",
    "                # Save previous file if exists\n",
    "                if current_file and current_content:\n",
    "                    self._create_file(repo_path, current_file, '\\n'.join(current_content))\n",
    "                    files_created.append(current_file)\n",
    "                \n",
    "                # Extract filename\n",
    "                current_file = line.replace('FILE:', '').strip()\n",
    "                current_content = []\n",
    "                \n",
    "                # Skip to code block\n",
    "                i += 1\n",
    "                while i < len(lines) and not lines[i].strip().startswith('```'):\n",
    "                    i += 1\n",
    "                \n",
    "                # Collect content until closing ```\n",
    "                i += 1\n",
    "                while i < len(lines) and not lines[i].strip().startswith('```'):\n",
    "                    current_content.append(lines[i])\n",
    "                    i += 1\n",
    "                    \n",
    "            i += 1\n",
    "        \n",
    "        # Save last file if exists\n",
    "        if current_file and current_content:\n",
    "            self._create_file(repo_path, current_file, '\\n'.join(current_content))\n",
    "            files_created.append(current_file)\n",
    "        \n",
    "        # Update state\n",
    "        created_files = self.get_state(\"created_files\", [])\n",
    "        created_files.extend(files_created)\n",
    "        self.update_state(\"created_files\", created_files)\n",
    "        \n",
    "        self.log(f\"✅ Successfully created {len(files_created)} files\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.log(f\"❌ Failed to apply code changes: {e}\", \"error\")\n",
    "        return False\n",
    "\n",
    "def _create_file(self, repo_path: str, filename: str, content: str) -> bool:\n",
    "    \"\"\"\n",
    "    Create a file in the repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_path: Repository path\n",
    "        filename: Name of the file to create\n",
    "        content: File content\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(repo_path, filename)\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        self.log(f\"✅ Created file: {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.log(f\"❌ Failed to create file {filename}: {e}\", \"error\")\n",
    "        return False\n",
    "\n",
    "# Add advanced file operation methods to the class\n",
    "AdvancedAIGeneratorAgent.get_model_info = get_model_info\n",
    "AdvancedAIGeneratorAgent.make_code_changes = make_code_changes\n",
    "AdvancedAIGeneratorAgent._create_file = _create_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:31:52] ℹ️ [AdvancedAIGenerator] 🤖 Initialized LangChain with OpenRouter model: openai/gpt-4o-mini\n",
      "🤖 Advanced AI Generator Agent created successfully!\n",
      "\n",
      "📊 Model Information:\n",
      "   • model: openai/gpt-4o-mini\n",
      "   • temperature: 0.1\n",
      "   • max_tokens: 32768\n",
      "   • supports_structured_output: True\n",
      "   • supports_reasoning: True\n",
      "   • context_window: 128000\n",
      "   • provider: OpenRouter\n",
      "   • langchain_integration: True\n",
      "   • state_management: True\n",
      "\n",
      "📁 Creating test repository: /Users/ishandutta/Documents/code/ai-accelerator/orion/ai_code_gen_demo\n",
      "✅ Created 3 sample files in test repository\n",
      "\n",
      "🔍 Repository Context Analysis:\n",
      "- Repository contains 2 code files (8 total lines)\n",
      "- Key files: utils.py, main.py\n",
      "- Project type: Python\n",
      "- Python project with pip dependencies\n"
     ]
    }
   ],
   "source": [
    "# Create the advanced generator\n",
    "advanced_generator = AdvancedAIGeneratorAgent(\n",
    "    model=\"openai/gpt-4o-mini\",  # Using GPT-4o-mini through OpenRouter\n",
    "    temperature=0.1,  # Low temperature for deterministic code generation\n",
    "    debug=True,  # Enable debug logging\n",
    ")\n",
    "\n",
    "print(\"🤖 Advanced AI Generator Agent created successfully!\")\n",
    "\n",
    "# Display model information\n",
    "model_info = advanced_generator.get_model_info()\n",
    "print(\"\\n📊 Model Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"   • {key}: {value}\")\n",
    "\n",
    "# Test repository context building\n",
    "test_repo = \"/Users/ishandutta/Documents/code/ai-accelerator/orion/ai_code_gen_demo\"\n",
    "print(f\"\\n📁 Creating test repository: {test_repo}\")\n",
    "\n",
    "# Create the test repository directory if it doesn't exist\n",
    "os.makedirs(test_repo, exist_ok=True)\n",
    "\n",
    "# Create some sample files for context testing\n",
    "sample_files = {\n",
    "    \"main.py\": \"# Main application file\\nif __name__ == '__main__':\\n    print('Hello World')\\n\",\n",
    "    \"utils.py\": \"# Utility functions\\ndef helper_function():\\n    pass\\n\",\n",
    "    \"requirements.txt\": \"requests==2.28.0\\nflask==2.2.0\\n\",\n",
    "}\n",
    "\n",
    "for filename, content in sample_files.items():\n",
    "    file_path = os.path.join(test_repo, filename)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"✅ Created {len(sample_files)} sample files in test repository\")\n",
    "\n",
    "# Test repository context building\n",
    "repo_context = advanced_generator.build_repository_context(test_repo)\n",
    "print(\"\\n🔍 Repository Context Analysis:\")\n",
    "print(repo_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Advanced Code Generation Demo\n",
    "\n",
    "Let's demonstrate the advanced code generation capabilities with repository-aware context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Advanced Generation Task: Create a FastAPI server that exposes an endpoint to generate images using a Stable Diffusion model from Hugging Face Diffusers. The server should accept a text prompt and return the generated image.\n",
      "📋 Context: {\n",
      "  \"framework\": \"FastAPI\",\n",
      "  \"diffusers_model\": \"runwayml/stable-diffusion-v1-5\",\n",
      "  \"features\": [\n",
      "    \"text-to-image endpoint\",\n",
      "    \"asynchronous image generation\",\n",
      "    \"input validation\",\n",
      "    \"return image as base64 or downloadable file\",\n",
      "    \"logging of requests\"\n",
      "  ],\n",
      "  \"requirements\": [\n",
      "    \"production-ready code\",\n",
      "    \"comprehensive error handling\",\n",
      "    \"security best practices (e.g., input sanitization, rate limiting)\",\n",
      "    \"proper project structure\",\n",
      "    \"GPU support if available\"\n",
      "  ],\n",
      "  \"dependencies\": [\n",
      "    \"diffusers\",\n",
      "    \"torch\",\n",
      "    \"transformers\",\n",
      "    \"fastapi\",\n",
      "    \"uvicorn\",\n",
      "    \"pillow\"\n",
      "  ]\n",
      "}\n",
      "⏳ Generating with GPT  and repository awareness...\n",
      "[12:33:15] ℹ️ [AdvancedAIGenerator] Starting operation: generate_code_changes\n",
      "[12:33:34] ✅ [AdvancedAIGenerator] Operation generate_code_changes completed in 18.53s\n",
      "\n",
      "✅ Advanced code generation successful!\n",
      "✅ Success: True\n",
      "📄 Result length: 2900 characters\n",
      "\n",
      "📝 Applying generated code to repository...\n",
      "[12:33:34] ℹ️ [AdvancedAIGenerator] 📝 Applying generated code changes to repository\n",
      "[12:33:34] ℹ️ [AdvancedAIGenerator] ✅ Created file: main.py\n",
      "[12:33:34] ℹ️ [AdvancedAIGenerator] ✅ Created file: requirements.txt\n",
      "[12:33:34] ℹ️ [AdvancedAIGenerator] ✅ Successfully created 2 files\n",
      "✅ Code changes applied successfully!\n",
      "\n",
      "📁 Files created: 2\n",
      "   • main.py\n",
      "   • requirements.txt\n",
      "\n",
      "📂 Updated repository structure:\n",
      "ai_code_gen_demo/\n",
      "  requirements.txt (97 bytes)\n",
      "  utils.py (52 bytes)\n",
      "  main.py (1537 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Advanced code generation task with rich context (Diffusers example)\n",
    "advanced_prompt = (\n",
    "    \"Create a FastAPI server that exposes an endpoint to generate images using a Stable Diffusion model from Hugging Face Diffusers. The server should accept a text prompt and return the generated image.\"\n",
    ")\n",
    "\n",
    "# Provide comprehensive context\n",
    "advanced_context = {\n",
    "    \"framework\": \"FastAPI\",\n",
    "    \"diffusers_model\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"features\": [\n",
    "        \"text-to-image endpoint\",\n",
    "        \"asynchronous image generation\",\n",
    "        \"input validation\",\n",
    "        \"return image as base64 or downloadable file\",\n",
    "        \"logging of requests\",\n",
    "    ],\n",
    "    \"requirements\": [\n",
    "        \"production-ready code\",\n",
    "        \"comprehensive error handling\",\n",
    "        \"security best practices (e.g., input sanitization, rate limiting)\",\n",
    "        \"proper project structure\",\n",
    "        \"GPU support if available\",\n",
    "    ],\n",
    "    \"dependencies\": [\n",
    "        \"diffusers\",\n",
    "        \"torch\",\n",
    "        \"transformers\",\n",
    "        \"fastapi\",\n",
    "        \"uvicorn\",\n",
    "        \"pillow\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(f\"🎯 Advanced Generation Task: {advanced_prompt}\")\n",
    "print(f\"📋 Context: {json.dumps(advanced_context, indent=2)}\")\n",
    "print(\"⏳ Generating with GPT  and repository awareness...\")\n",
    "\n",
    "# Generate code with advanced features\n",
    "result = advanced_generator.generate_code_changes(\n",
    "    prompt=advanced_prompt, repo_path=test_repo, context=advanced_context\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\n✅ Advanced code generation successful!\")\n",
    "\n",
    "    # Show generation statistics\n",
    "    generation_history = advanced_generator.get_state(\"generated_code\", [])\n",
    "    if generation_history:\n",
    "        latest = generation_history[-1]\n",
    "        print(f\"✅ Success: {latest.get('success', False)}\")\n",
    "        print(f\"📄 Result length: {len(result)} characters\")\n",
    "\n",
    "    # Apply the generated code to the repository\n",
    "    print(\"\\n📝 Applying generated code to repository...\")\n",
    "    success = advanced_generator.make_code_changes(result, test_repo)\n",
    "\n",
    "    if success:\n",
    "        print(\"✅ Code changes applied successfully!\")\n",
    "\n",
    "        # Show created files\n",
    "        created_files = advanced_generator.get_state(\"created_files\", [])\n",
    "        print(f\"\\n📁 Files created: {len(created_files)}\")\n",
    "        for file in created_files:\n",
    "            print(f\"   • {file}\")\n",
    "\n",
    "        # Show repository structure\n",
    "        print(\"\\n📂 Updated repository structure:\")\n",
    "        for root, dirs, files in os.walk(test_repo):\n",
    "            level = root.replace(test_repo, \"\").count(os.sep)\n",
    "            indent = \"  \" * level\n",
    "            folder_name = os.path.basename(root) or \"root\"\n",
    "            print(f\"{indent}{folder_name}/\")\n",
    "\n",
    "            subindent = \"  \" * (level + 1)\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                print(f\"{subindent}{file} ({size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Modification Demo\n",
    "\n",
    "Let's demonstrate the code modification capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Modification Demo\n",
      "📁 Created sample file: calculator_sample.py\n",
      "📄 Original file size: 555 characters\n",
      "\n",
      "🎯 Modification request: Update the version number from 1.0.0 to 2.1.0 and add a multiply method\n",
      "⏳ Applying modification...\n",
      "[12:35:54] ℹ️ [AdvancedAIGenerator] 🔧 Modifying file with advanced model: calculator_sample.py\n",
      "[12:35:54] ℹ️ [AdvancedAIGenerator] 📄 Original file content (555 chars)\n",
      "[12:36:01] ℹ️ [AdvancedAIGenerator] ✅ Advanced model modification applied (similarity: 0.90)\n",
      "[12:36:01] ℹ️ [AdvancedAIGenerator] ✅ Modified file: /Users/ishandutta/Documents/code/ai-accelerator/orion/ai_code_gen_demo/calculator_sample.py\n",
      "\n",
      "✅ Modification successful!\n",
      "📄 Modified file size: 665 characters\n",
      "🔄 Similarity score: 90.49%\n",
      "📊 Size change: 555 → 665 chars\n",
      "\n",
      "📝 Content comparison (preview):\n",
      "ORIGINAL (first 300 chars):\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Sample application for testing modifications.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "class Calculator:\n",
      "    \"\"\"A simple calculator class.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.version = \"1.0.0\"\n",
      "\n",
      "    def add(self, a: float, b: float) -> float:\n",
      "        \"\"\"Add two numbers.\"\"\"\n",
      "        r...\n",
      "\n",
      "MODIFIED (first 300 chars):\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Sample application for testing modifications.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import sys\n",
      "\n",
      "class Calculator:\n",
      "    \"\"\"A simple calculator class.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.version = \"2.1.0\"\n",
      "\n",
      "    def add(self, a: float, b: float) -> float:\n",
      "        \"\"\"Add two numbers.\"\"\"\n",
      "        r...\n"
     ]
    }
   ],
   "source": [
    "if advanced_generator and test_repo:\n",
    "    # Create a sample file to modify\n",
    "    sample_code = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Sample application for testing modifications.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class Calculator:\n",
    "    \"\"\"A simple calculator class.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.version = \"1.0.0\"\n",
    "    \n",
    "    def add(self, a: float, b: float) -> float:\n",
    "        \"\"\"Add two numbers.\"\"\"\n",
    "        return a + b\n",
    "    \n",
    "    def subtract(self, a: float, b: float) -> float:\n",
    "        \"\"\"Subtract two numbers.\"\"\"\n",
    "        return a - b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    calc = Calculator()\n",
    "    print(f\"Calculator v{calc.version}\")\n",
    "    print(f\"2 + 3 = {calc.add(2, 3)}\")\n",
    "'''\n",
    "\n",
    "    # Write the sample file\n",
    "    sample_file_path = os.path.join(test_repo, \"calculator_sample.py\")\n",
    "    with open(sample_file_path, \"w\") as f:\n",
    "        f.write(sample_code)\n",
    "\n",
    "    print(\"🔧 Modification Demo\")\n",
    "    print(f\"📁 Created sample file: calculator_sample.py\")\n",
    "    print(f\"📄 Original file size: {len(sample_code)} characters\")\n",
    "\n",
    "    # Test modification\n",
    "    modification_request = (\n",
    "        \"Update the version number from 1.0.0 to 2.1.0 and add a multiply method\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n🎯 Modification request: {modification_request}\")\n",
    "    print(\"⏳ Applying modification...\")\n",
    "\n",
    "    modified_content = advanced_generator.modify_existing_file(\n",
    "        repo_path=test_repo,\n",
    "        file_path=\"calculator_sample.py\",\n",
    "        modification_description=modification_request,\n",
    "    )\n",
    "\n",
    "    if modified_content:\n",
    "        print(\"\\n✅ Modification successful!\")\n",
    "        print(f\"📄 Modified file size: {len(modified_content)} characters\")\n",
    "\n",
    "        # Show modification statistics\n",
    "        modified_files = advanced_generator.get_state(\"modified_files\", [])\n",
    "        if modified_files:\n",
    "            latest_mod = modified_files[-1]\n",
    "            print(f\"🔄 Similarity score: {latest_mod.get('similarity', 0):.2%}\")\n",
    "            print(\n",
    "                f\"📊 Size change: {latest_mod.get('original_size', 0)} → {latest_mod.get('modified_size', 0)} chars\"\n",
    "            )\n",
    "\n",
    "        # Show the differences (first 500 characters of each)\n",
    "        print(\"\\n📝 Content comparison (preview):\")\n",
    "        print(\"ORIGINAL (first 300 chars):\")\n",
    "        print(sample_code[:300] + \"...\")\n",
    "        print(\"\\nMODIFIED (first 300 chars):\")\n",
    "        print(modified_content[:300] + \"...\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Modification failed!\")\n",
    "else:\n",
    "    print(\"⚠️ Modification demo skipped (requires OpenAI API key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Advanced Analytics and State Management\n",
    "\n",
    "Let's explore the comprehensive state management and analytics capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Advanced Analytics Dashboard\n",
      "==================================================\n",
      "🤖 Model: openai/gpt-4o-mini\n",
      "🌡️ Temperature: 0.1\n",
      "📏 Context Window: 128,000 tokens\n",
      "🔗 LangChain: True\n",
      "\n",
      "📈 Generation History: 1 generations\n",
      "\n",
      "1. 🎯 Generation 1\n",
      "   📝 Prompt: Create a FastAPI server that exposes an endpoint t...\n",
      "   ✅ Success: True\n",
      "   ⏰ Time: Sat Sep 27 12:33:34 2025\n",
      "\n",
      "📁 File Operations Summary:\n",
      "   📄 Files created: 2\n",
      "   🔧 Files modified: 2\n",
      "\n",
      "📝 Created Files:\n",
      "   • main.py\n",
      "   • requirements.txt\n",
      "\n",
      "🔧 Modified Files:\n",
      "   • calculator_sample.py (similarity: 90.5%, Sat Sep 27 12:35:34 2025)\n",
      "   • calculator_sample.py (similarity: 90.5%, Sat Sep 27 12:36:01 2025)\n",
      "\n",
      "🔍 State Overview:\n",
      "   • model: openai/gpt-4o-mini\n",
      "   • temperature: 0.1\n",
      "   • generated_code: 1 items\n",
      "   • created_files: 2 items\n",
      "   • modified_files: 2 items\n",
      "\n",
      "🌟 Advanced Features Demonstrated:\n",
      "   ✅ GPT-4o-mini integration with validation\n",
      "   ✅ LangChain orchestration\n",
      "   ✅ Repository-aware context building\n",
      "   ✅ Code modification\n",
      "   ✅ Advanced state management\n",
      "   ✅ Comprehensive analytics\n",
      "   ✅ Production error handling\n"
     ]
    }
   ],
   "source": [
    "if advanced_generator:\n",
    "    print(\"📊 Advanced Analytics Dashboard\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Model information\n",
    "    model_info = advanced_generator.get_model_info()\n",
    "    print(f\"🤖 Model: {model_info['model']}\")\n",
    "    print(f\"🌡️ Temperature: {model_info['temperature']}\")\n",
    "    print(f\"📏 Context Window: {model_info['context_window']:,} tokens\")\n",
    "    print(f\"🔗 LangChain: {model_info['langchain_integration']}\")\n",
    "\n",
    "    # Generation history\n",
    "    generation_history = advanced_generator.get_state(\"generated_code\", [])\n",
    "    print(f\"\\n📈 Generation History: {len(generation_history)} generations\")\n",
    "\n",
    "    if generation_history:\n",
    "        for i, gen in enumerate(generation_history, 1):\n",
    "            print(f\"\\n{i}. 🎯 Generation {i}\")\n",
    "            print(f\"   📝 Prompt: {gen.get('prompt', 'Unknown')[:50]}...\")\n",
    "            print(f\"   ✅ Success: {gen.get('success', False)}\")\n",
    "            print(f\"   ⏰ Time: {time.ctime(gen.get('timestamp', 0))}\")\n",
    "\n",
    "    # File operations\n",
    "    created_files = advanced_generator.get_state(\"created_files\", [])\n",
    "    modified_files = advanced_generator.get_state(\"modified_files\", [])\n",
    "\n",
    "    print(f\"\\n📁 File Operations Summary:\")\n",
    "    print(f\"   📄 Files created: {len(created_files)}\")\n",
    "    print(f\"   🔧 Files modified: {len(modified_files)}\")\n",
    "\n",
    "    if created_files:\n",
    "        print(f\"\\n📝 Created Files:\")\n",
    "        for file in created_files:\n",
    "            print(f\"   • {file}\")\n",
    "\n",
    "    if modified_files:\n",
    "        print(f\"\\n🔧 Modified Files:\")\n",
    "        for mod in modified_files:\n",
    "            similarity = mod.get(\"similarity\", 0)\n",
    "            timestamp = time.ctime(mod.get(\"timestamp\", 0))\n",
    "            print(\n",
    "                f\"   • {os.path.basename(mod.get('path', 'unknown'))} (similarity: {similarity:.1%}, {timestamp})\"\n",
    "            )\n",
    "\n",
    "    # State overview\n",
    "    print(f\"\\n🔍 State Overview:\")\n",
    "    for key, value in advanced_generator.state.items():\n",
    "        if isinstance(value, list):\n",
    "            print(f\"   • {key}: {len(value)} items\")\n",
    "        else:\n",
    "            print(f\"   • {key}: {value}\")\n",
    "\n",
    "    print(\"\\n🌟 Advanced Features Demonstrated:\")\n",
    "    print(\"   ✅ GPT-4o-mini integration with validation\")\n",
    "    print(\"   ✅ LangChain orchestration\")\n",
    "    print(\"   ✅ Repository-aware context building\")\n",
    "    print(\"   ✅ Code modification\")\n",
    "    print(\"   ✅ Advanced state management\")\n",
    "    print(\"   ✅ Comprehensive analytics\")\n",
    "    print(\"   ✅ Production error handling\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Analytics dashboard not available (requires OpenAI API key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Advanced Tutorial Conclusion\n",
    "\n",
    "Congratulations! You've completed **Part 2B: Advanced Production Features** of the AI Generator Agent tutorial. Here's what you've mastered:\n",
    "\n",
    "### ✅ Production Architecture Mastered\n",
    "\n",
    "1. **🔗 LangChain Integration**: Professional AI workflow orchestration with chains and parsers\n",
    "2. **🤖 GPT-5 Optimization**: Advanced model configuration, validation, and feature detection\n",
    "3. **🧠 Repository Intelligence**: Automated codebase analysis and context building\n",
    "4. **🔧 Modifications**: Precise file editing with validation and safety checks\n",
    "5. **🛡️ Production Resilience**: Fallback systems, error handling, and state management\n",
    "\n",
    "### 🛠️ Advanced Technical Skills\n",
    "\n",
    "- **LangChain Orchestration**: Building complex AI workflows with templates and chains\n",
    "- **GPT-5 Integration**: Leveraging advanced model capabilities and structured outputs\n",
    "- **Repository Analysis**: Intelligent codebase scanning and context extraction\n",
    "- **State Management**: Persistent tracking of operations and analytics\n",
    "- **Similarity Validation**: Safe code modification with change verification\n",
    "- **Production Logging**: Comprehensive tracking and monitoring systems\n",
    "\n",
    "### 🎯 Production Best Practices\n",
    "\n",
    "- **Structured Output Validation**: Use Pydantic models for reliable AI responses\n",
    "- **Context-Aware Generation**: Leverage repository analysis for better results\n",
    "- **Precision**: Make minimal, targeted changes to preserve functionality\n",
    "- **Fallback Systems**: Implement graceful degradation for resilience\n",
    "- **State Persistence**: Track operations for analytics and debugging\n",
    "- **Security Consciousness**: Follow best practices in generated code\n",
    "\n",
    "### 🚀 Ready for Enterprise\n",
    "\n",
    "You're now equipped to:\n",
    "- Build production-ready AI code generation systems\n",
    "- Integrate with existing development workflows\n",
    "- Implement multi-agent AI systems like Orion\n",
    "- Develop domain-specific AI agents\n",
    "- Create enterprise-grade AI development tools\n",
    "\n",
    "### 🔄 Connection to Orion System\n",
    "\n",
    "This tutorial demonstrates the core architecture used in the Orion AI Agent System:\n",
    "- **BaseAgent Integration**: State management and execution tracking\n",
    "- **Multi-Agent Context**: Repository scanner and task classifier integration\n",
    "- **LangGraph Orchestration**: Complex workflow coordination (covered in Tutorial 3)\n",
    "- **Production Operations**: File management, testing, and deployment\n",
    "\n",
    "---\n",
    "\n",
    "*Continue to Tutorial 3 (LangGraph Orchestration) to learn how multiple AI agents work together in complex workflows!*\n",
    "\n",
    "### 📚 Tutorial Series Navigation\n",
    "\n",
    "- ✅ **Part 1**: GitHub Integration & Automation\n",
    "- ✅ **Part 2A**: AI Generator Core Concepts  \n",
    "- ✅ **Part 2B**: AI Generator Advanced Features (This Tutorial)\n",
    "- 🔄 **Part 3**: LangGraph Orchestration (Next)\n",
    "- 🔄 **Part 4**: Repository Scanner & Task Classifier\n",
    "- 🔄 **Part 5**: Environment Management & Testing\n",
    "- 🔄 **Part 6**: Complete System Integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accelerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
