{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eng-accelerator/ai-accelerator/blob/main/Day_8/day_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZHCED27WVTZ"
      },
      "source": [
        "# Install and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSzESzpKUrBE"
      },
      "outputs": [],
      "source": [
        "! pip install transformers diffusers pillow gradio torch datasets evaluate accelerate ftfy pyarrow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7XVhhoHWYGE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline,\n",
        "    StableDiffusionPipeline,\n",
        "    StableDiffusionImg2ImgPipeline,\n",
        "    StableDiffusionInpaintPipeline,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    infer_device,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WACww2q3W2CS"
      },
      "source": [
        "# Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSsXmNbtW4mS"
      },
      "source": [
        "### Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F65No9BtWzIh"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"C++ is \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPZiy2peW_ro"
      },
      "source": [
        "### Do more with pipeline - explore yourself!\n",
        "\n",
        "**How to experiment.**\n",
        "\n",
        "\n",
        "Two places -- while creating the [pipeline](https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/pipelines/__init__.py#L637) and while generating the actual output (text image etc.)\n",
        "\n",
        "\n",
        "List of tasks that can be done with pipeline - figure this out. Experiment with each yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgl4jlZ9W0qW"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "generator(\"C++ is\",\n",
        "            max_new_tokens=5,\n",
        "            num_return_sequences=2,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVk-HKrpXQJ6"
      },
      "source": [
        "### Images and pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6PLOJgVXQsm"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "image_classifier = pipeline(\n",
        "    task=\"image-classification\", model=\"google/vit-base-patch16-224\"\n",
        ")\n",
        "result = image_classifier(\n",
        "    \"https://media.istockphoto.com/id/1443562748/photo/cute-ginger-cat.jpg?s=1024x1024&w=is&k=20&c=QaEkKC7lFEBrzzPftMRBVuOZq4FNOnUjOV1VqTmpMfY=\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9vJYNJBXYbs"
      },
      "source": [
        "### Multimodal example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OzLbKdGXY5E"
      },
      "outputs": [],
      "source": [
        "vqa_pipeline = pipeline(\n",
        "    \"visual-question-answering\", model=\"Salesforce/blip-vqa-capfilt-large\"\n",
        ")\n",
        "\n",
        "image = Image.open(\"baby_goat.jpg\")\n",
        "question = \"Is there an elephant?\"\n",
        "\n",
        "vqa_pipeline(image, question, top_k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u88GkgdQXgh8"
      },
      "source": [
        "Gradio app!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJIK1wVcXbdM"
      },
      "outputs": [],
      "source": [
        "def answer_question(image, question):\n",
        "    if image is None or question.strip() == \"\":\n",
        "        return \"Please provide an image and a question.\"\n",
        "    outputs = vqa_pipeline(image, question, top_k=1)\n",
        "    return outputs[0][\"answer\"]\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üñºÔ∏è Visual Question Answering with BLIP\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload an Image\")\n",
        "            question_input = gr.Textbox(label=\"Enter your question\")\n",
        "            submit_btn = gr.Button(\"Get Answer\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=answer_question,\n",
        "        inputs=[image_input, question_input],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOvZ_-hiXpAP"
      },
      "source": [
        "# Auto-classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjEaydzNXvtB"
      },
      "source": [
        "### AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDlEVyIlXjDP"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I am learning Operating Systems and it's so fun.\",\n",
        "    \"I write terrible C++ code!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=15)\n",
        "print(inputs)\n",
        "\n",
        "# another way to do it - a closer look into the tokenizer\n",
        "sequence = \"I am learning Operating Systems and it's so fun.\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(\"--------TOKENIZED SENTENCE--------\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n\\n--------TOKENS MAPPED TO THEIR IDS--------\")\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltIeH2b4X6IA"
      },
      "source": [
        "**TAKE A PAUSE!**\n",
        "\n",
        "**SPECIAL TOKENS!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k4qI1FIX6il"
      },
      "outputs": [],
      "source": [
        "# already tokenized input\n",
        "decoded_string = tokenizer.decode([1045, 2572, 4083, 4082, 3001, 1998, 2009, 1005, 1055, 2061, 4569, 1012])\n",
        "print(f\"DECODING input_ids of ALREADY TOKENIZED INPUT:\\n{decoded_string}\\n\\n\")\n",
        "\n",
        "# raw text\n",
        "decoded_string = tokenizer.decode([101, 1045, 2572, 4083, 4082, 3001, 1998, 2009, 1005, 1055, 2061, 4569,\n",
        "         1012,  102])\n",
        "print(f\"DECODING input_ids of RAW TOKENIZED INPUT:\\n{decoded_string}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spFyi0RJY6zc"
      },
      "source": [
        "**Padding - ways to do it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd1QsUVYY7Ti"
      },
      "outputs": [],
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tjMnJugZENE"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSz70KBgZFJn"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "print(f\"shape of output tensor from distilbert-base-uncased-finetuned-sst-2-english:\\n{outputs.last_hidden_state.shape}\\n\\n\")\n",
        "# bs, seq_len, dim\n",
        "# torch.Size([2, 14, 768])\n",
        "\n",
        "\n",
        "# loading the model with the appropriate task class\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "print(\"--------SHAPE OF LOGIT TENSOR--------\")\n",
        "print(outputs.logits.shape)\n",
        "\n",
        "print(\"\\n\\n--------LOGIT TENSOR--------\")\n",
        "print(outputs.logits)\n",
        "\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "predictions = torch.argmax(predictions, dim=1)\n",
        "print(\"\\n\\n--------PREDICTIONS--------\")\n",
        "print(predictions)\n",
        "print(f\"labels according to the model config:\\n{model.config.id2label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92YgnBKRaHh_"
      },
      "source": [
        "# Fast inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUc92JMRaMRj"
      },
      "source": [
        "### Precision (dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMhf_sQBaKDc"
      },
      "outputs": [],
      "source": [
        "### Precision (dtype)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWRJW1qaPx6"
      },
      "source": [
        "### torch.compile + channels last + dynamic compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5E0iTVpaQLX"
      },
      "outputs": [],
      "source": [
        "torch._inductor.config.conv_1x1_as_mm = True\n",
        "torch._inductor.config.coordinate_descent_tuning = True\n",
        "torch._inductor.config.epilogue_fusion = False\n",
        "torch._inductor.config.coordinate_descent_check_all_directions = True\n",
        "\n",
        "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "pipeline.unet.to(memory_format=torch.channels_last)\n",
        "pipeline.vae.to(memory_format=torch.channels_last)\n",
        "pipeline.unet = torch.compile(\n",
        "    pipeline.unet, mode=\"max-autotune\", fullgraph=True, dynamic=True\n",
        ")\n",
        "pipeline.vae.decode = torch.compile(\n",
        "    pipeline.vae.decode,\n",
        "    mode=\"max-autotune\",\n",
        "    fullgraph=True, dynamic=True\n",
        ")\n",
        "\n",
        "prompt = \"A rabbit in a garden, warm and muted colors, detailed, 8k\"\n",
        "pipeline(prompt, num_inference_steps=30).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GediuB2LaUPa"
      },
      "source": [
        "### Speculative decoding\n",
        "\n",
        "The assistant and LLM model must also share the same tokenizer to avoid re-encoding and decoding tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9csvl4CMaUlL"
      },
      "outputs": [],
      "source": [
        "device = infer_device()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "inputs = tokenizer(\"just some sample text\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n",
        "assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.7)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3srQ5ngaYHP"
      },
      "source": [
        "### Prompt lookup decoding\n",
        "\n",
        "works best for input-grounded tasks (eg. summarization) where there's overlapping words between the input and output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdI5Qqj1aYuS"
      },
      "outputs": [],
      "source": [
        "device = infer_device()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "inputs = tokenizer(\"more sample text\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n",
        "assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
        "outputs = model.generate(**inputs, prompt_lookup_num_tokens=3)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by1QIYroacKq"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x4rukJsadnb"
      },
      "outputs": [],
      "source": [
        "# flash attn 2\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# option 1\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2b\",\n",
        "    quantization_config=quant_config,\n",
        "    dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "# option 2\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2b\",\n",
        "    quantization_config=quant_config,\n",
        "    dtype=torch.bfloat16\n",
        ")\n",
        "model.set_attention_implementation(\"flash_attention_2\")\n",
        "\n",
        "\n",
        "\n",
        "# torch sdpa\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2b\",\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
        "    outputs = model.generate(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW_49fI4a_lR"
      },
      "source": [
        "# Accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKBlz0_9bAzH"
      },
      "outputs": [],
      "source": [
        "# the code below is just for deomonstration and understanding purposes - it won't run unless a PyTorch model MyModelClass is defined, trained and saved in a file called ckpt_file\n",
        "#################################### FOCUS HERE ################################\n",
        "\n",
        "# just a skeleton\n",
        "with init_empty_weights():\n",
        "    model = MyModelClass(...)\n",
        "\n",
        "model = load_checkpoint_and_dispatch(\n",
        "    model, checkpoint=ckpt_file, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "################################################################################\n",
        "\n",
        "input = torch.randn(2,3)\n",
        "device_type = next(iter(model.parameters())).device.type\n",
        "input = input.to(device_type)\n",
        "output = model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HnlgfybbKyB"
      },
      "source": [
        "\n",
        "That was PyTorch. What about HF models (using accelerate w the autoclasses API we just studied)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKZl_vO5bOua"
      },
      "outputs": [],
      "source": [
        "# this would work\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyX3zjrhbR1p"
      },
      "source": [
        "# Diffusers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT7ImrX-bTtl"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------\n",
        "# 1. TEXT-TO-IMAGE GENERATION\n",
        "# -------------------------------------\n",
        "\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
        ").to(torch_device)\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"A majestic lion wearing a crown, photorealistic, 4k, highly detailed\"\n",
        "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
        "\n",
        "display(image)\n",
        "\n",
        "\"\"\"\n",
        "üìù Parameters explained:\n",
        "- prompt: text description of the image\n",
        "- num_inference_steps: how many denoising steps (higher = better quality, slower)\n",
        "- guidance_scale: how strongly the prompt guides generation (7-8 is common; higher = more prompt fidelity, lower = more creativity)\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgHeFCcGbtrG"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------\n",
        "# 2. IMAGE-TO-IMAGE GENERATION\n",
        "# -------------------------------------\n",
        "\n",
        "img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
        ").to(torch_device)\n",
        "\n",
        "\n",
        "url = \"https://images.unsplash.com/photo-1480497490787-505ec076689f?q=80&w=2069&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        "init_image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\").resize((512, 512))\n",
        "\n",
        "prompt = \"A futuristic city skyline painted on the mountain\"\n",
        "strength = 0.7  # how much noise to add: 0 = almost same as input, 1 = ignore input\n",
        "num_inference_steps = 40\n",
        "\n",
        "img2img = img2img_pipe(\n",
        "    prompt=prompt,\n",
        "    image=init_image,\n",
        "    strength=strength,\n",
        "    num_inference_steps=num_inference_steps,\n",
        "    guidance_scale=7.5\n",
        ").images[0]\n",
        "\n",
        "display(init_image)\n",
        "display(img2img)\n",
        "\n",
        "\"\"\"\n",
        "üìù Parameters explained:\n",
        "- image: input image you want to transform\n",
        "- strength: controls how much noise is added\n",
        "    - low strength (0.2-0.4): keeps input structure, small edits\n",
        "    - high strength (0.7-0.9): more creative, diverges from input\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v13e5q-bw86"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------\n",
        "# 3. INPAINTING\n",
        "# -------------------------------------\n",
        "\n",
        "inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16\n",
        ").to(torch_device)\n",
        "\n",
        "\n",
        "url = \"https://images.unsplash.com/photo-1480497490787-505ec076689f?q=80&w=2069&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        "init_image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\").resize((512, 512))\n",
        "\n",
        "# Mask: white = keep, black = replace with noise\n",
        "mask_url = \"https://images.unsplash.com/photo-1573865526739-10659fec78a5?q=80&w=1015&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        "mask_image = Image.open(BytesIO(requests.get(mask_url).content)).convert(\"RGB\").resize((512, 512))\n",
        "\n",
        "prompt = \"Replace the masked area with a cute cat sitting there\"\n",
        "inpaint = inpaint_pipe(\n",
        "    prompt=prompt,\n",
        "    image=init_image,\n",
        "    mask_image=mask_image,\n",
        "    num_inference_steps=40,\n",
        "    guidance_scale=7.5\n",
        ").images[0]\n",
        "\n",
        "display(init_image)\n",
        "display(mask_image)\n",
        "display(inpaint)\n",
        "\n",
        "\"\"\"\n",
        "üìù Parameters explained:\n",
        "- image: input image\n",
        "- mask_image: white = preserved areas, black = replaced by generation\n",
        "- num_inference_steps: steps of denoising (same as before)\n",
        "- guidance_scale: prompt strength (same as before)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuCuTmPDbzZS"
      },
      "source": [
        "### Schedulers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEplv8cwb03I"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# 4. EXPLORING DIFFERENT SCHEDULERS\n",
        "# =====================================\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"A futuristic cyberpunk cityscape at night, neon lights, highly detailed\"\n",
        "num_inference_steps = 30\n",
        "guidance_scale = 7.5\n",
        "\n",
        "schedulers = {\n",
        "    \"Euler\": EulerDiscreteScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\"),\n",
        "    \"Euler Ancestral\": EulerAncestralDiscreteScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\"),\n",
        "    \"DDIM\": DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\"),\n",
        "    \"LMS\": LMSDiscreteScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\"),\n",
        "    \"DPM-Solver++\": DPMSolverMultistepScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\"),\n",
        "}\n",
        "\n",
        "images = {}\n",
        "\n",
        "for name, scheduler in schedulers.items():\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        scheduler=scheduler,\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(torch_device)\n",
        "\n",
        "    print(f\"\\n\\n---------------------------Generating with {name} scheduler---------------------------------\")\n",
        "    image = pipe(\n",
        "        prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale\n",
        "    ).images[0]\n",
        "\n",
        "    images[name] = image\n",
        "    display(image)\n",
        "\n",
        "\"\"\"\n",
        "üìù Key Notes on Schedulers:\n",
        "\n",
        "1. Euler:\n",
        "   - Fast and stable\n",
        "   - Great default choice\n",
        "   - Good balance of quality and speed\n",
        "\n",
        "2. Euler Ancestral:\n",
        "   - More creative / adds variation\n",
        "   - Can be less deterministic\n",
        "   - Useful when you want diverse samples\n",
        "\n",
        "3. DDIM (Denoising Diffusion Implicit Model):\n",
        "   - Deterministic (same seed ‚Üí same output)\n",
        "   - Faster sampling\n",
        "   - Slightly less detailed sometimes\n",
        "\n",
        "4. LMS (Laplacian Pyramid Solver):\n",
        "   - Sharp results\n",
        "   - Popular for high-quality outputs\n",
        "   - Slower than Euler\n",
        "\n",
        "5. DPM-Solver++:\n",
        "   - Modern, very efficient\n",
        "   - High-quality images in fewer steps\n",
        "   - Good for faster inference with quality\n",
        "\n",
        "üëâ Tip: Try lowering num_inference_steps (e.g. 15) with DPM-Solver++ and compare!\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOmWQ0z7Ok4MOtl6V067BuC",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
