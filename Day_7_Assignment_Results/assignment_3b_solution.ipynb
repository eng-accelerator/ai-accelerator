{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3b Solution: Advanced Gradio RAG Frontend\n",
        "## Day 6 Session 2 - Building Configurable RAG Applications\n",
        "\n",
        "This notebook contains the complete solution for Assignment 3b.\n",
        "\n",
        "**Solution demonstrates:**\n",
        "- Advanced Gradio layout with columns and complex controls\n",
        "- All RAG configuration parameters using OpenRouter (not OpenAI)\n",
        "- Professional UI patterns and component organization\n",
        "- Dynamic parameter handling and real-time configuration display\n",
        "\n",
        "**Note:** This solution uses OpenRouter for LLM access. Make sure you have your `OPENROUTER_API_KEY` environment variable set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import gradio as gr\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# LlamaIndex core components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openrouter import OpenRouter  # Using OpenRouter, not OpenAI\n",
        "\n",
        "# Advanced RAG components\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Advanced RAG Backend initialized and ready!\n"
          ]
        }
      ],
      "source": [
        "class AdvancedRAGBackend:\n",
        "    \"\"\"Advanced RAG backend with configurable parameters.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.available_models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "        self.available_postprocessors = [\"SimilarityPostprocessor\"]\n",
        "        self.available_synthesizers = [\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n",
        "        self.update_settings()\n",
        "        \n",
        "    def update_settings(self, model: str = \"gpt-4o-mini\", temperature: float = 0.1, chunk_size: int = 512, chunk_overlap: int = 50):\n",
        "        \"\"\"Update LlamaIndex settings based on user configuration.\"\"\"\n",
        "        # Use OpenRouter API key (not OpenAI)\n",
        "        api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "        if api_key:\n",
        "            Settings.llm = OpenRouter(api_key=api_key, model=model, temperature=temperature)\n",
        "        Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", trust_remote_code=True)\n",
        "        Settings.chunk_size = chunk_size\n",
        "        Settings.chunk_overlap = chunk_overlap\n",
        "    \n",
        "    def initialize_database(self, data_folder=\"../data\"):\n",
        "        \"\"\"Initialize the vector database with documents.\"\"\"\n",
        "        if not Path(data_folder).exists():\n",
        "            return f\"‚ùå Data folder '{data_folder}' not found!\"\n",
        "        \n",
        "        try:\n",
        "            vector_store = LanceDBVectorStore(uri=\"./advanced_rag_vectordb\", table_name=\"documents\")\n",
        "            reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
        "            documents = reader.load_data()\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            self.index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, show_progress=True)\n",
        "            return f\"‚úÖ Database initialized successfully with {len(documents)} documents!\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error initializing database: {str(e)}\"\n",
        "    \n",
        "    def get_postprocessor(self, postprocessor_name: str, similarity_cutoff: float):\n",
        "        \"\"\"Get the selected postprocessor.\"\"\"\n",
        "        if postprocessor_name == \"SimilarityPostprocessor\":\n",
        "            return SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
        "        return None\n",
        "    \n",
        "    def get_synthesizer(self, synthesizer_name: str):\n",
        "        \"\"\"Get the selected response synthesizer.\"\"\"\n",
        "        synthesizers = {\n",
        "            \"TreeSummarize\": TreeSummarize(),\n",
        "            \"Refine\": Refine(),\n",
        "            \"CompactAndRefine\": CompactAndRefine()\n",
        "        }\n",
        "        return synthesizers.get(synthesizer_name)\n",
        "    \n",
        "    def advanced_query(self, question: str, model: str, temperature: float, \n",
        "                      chunk_size: int, chunk_overlap: int, similarity_top_k: int,\n",
        "                      postprocessor_names: List[str], similarity_cutoff: float,\n",
        "                      synthesizer_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system with advanced configuration.\"\"\"\n",
        "        if self.index is None:\n",
        "            return {\"response\": \"‚ùå Please initialize the database first!\", \"sources\": [], \"config\": {}}\n",
        "        if not question or not question.strip():\n",
        "            return {\"response\": \"‚ö†Ô∏è Please enter a question first!\", \"sources\": [], \"config\": {}}\n",
        "        \n",
        "        try:\n",
        "            self.update_settings(model, temperature, chunk_size, chunk_overlap)\n",
        "            \n",
        "            postprocessors = [self.get_postprocessor(name, similarity_cutoff) \n",
        "                            for name in postprocessor_names if self.get_postprocessor(name, similarity_cutoff)]\n",
        "            synthesizer = self.get_synthesizer(synthesizer_name)\n",
        "            \n",
        "            kwargs = {\"similarity_top_k\": similarity_top_k}\n",
        "            if postprocessors: kwargs[\"node_postprocessors\"] = postprocessors\n",
        "            if synthesizer: kwargs[\"response_synthesizer\"] = synthesizer\n",
        "            \n",
        "            query_engine = self.index.as_query_engine(**kwargs)\n",
        "            response = query_engine.query(question)\n",
        "            \n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                sources = [{\"text\": node.text[:200] + \"...\", \"score\": getattr(node, 'score', 0.0),\n",
        "                          \"source\": getattr(node.node, 'metadata', {}).get('file_name', 'Unknown')}\n",
        "                         for node in response.source_nodes]\n",
        "            \n",
        "            return {\n",
        "                \"response\": str(response), \"sources\": sources,\n",
        "                \"config\": {\"model\": model, \"temperature\": temperature, \"chunk_size\": chunk_size,\n",
        "                          \"chunk_overlap\": chunk_overlap, \"similarity_top_k\": similarity_top_k,\n",
        "                          \"postprocessors\": postprocessor_names, \"similarity_cutoff\": similarity_cutoff,\n",
        "                          \"synthesizer\": synthesizer_name}\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"response\": f\"‚ùå Error processing query: {str(e)}\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "rag_backend = AdvancedRAGBackend()\n",
        "print(\"üöÄ Advanced RAG Backend initialized and ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Advanced RAG interface created successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_advanced_rag_interface():\n",
        "    \"\"\"Complete solution for advanced RAG interface.\"\"\"\n",
        "    \n",
        "    def initialize_db():\n",
        "        return rag_backend.initialize_database()\n",
        "    \n",
        "    def handle_advanced_query(question, model, temperature, chunk_size, chunk_overlap, \n",
        "                             similarity_top_k, postprocessors, similarity_cutoff, synthesizer):\n",
        "        result = rag_backend.advanced_query(\n",
        "            question, model, temperature, chunk_size, chunk_overlap,\n",
        "            similarity_top_k, postprocessors, similarity_cutoff, synthesizer\n",
        "        )\n",
        "        \n",
        "        config_text = f\"\"\"**Current Configuration:**\n",
        "- Model: {result['config'].get('model', 'N/A')}\n",
        "- Temperature: {result['config'].get('temperature', 'N/A')}\n",
        "- Chunk Size: {result['config'].get('chunk_size', 'N/A')}\n",
        "- Chunk Overlap: {result['config'].get('chunk_overlap', 'N/A')}\n",
        "- Similarity Top-K: {result['config'].get('similarity_top_k', 'N/A')}\n",
        "- Postprocessors: {', '.join(result['config'].get('postprocessors', []))}\n",
        "- Similarity Cutoff: {result['config'].get('similarity_cutoff', 'N/A')}\n",
        "- Synthesizer: {result['config'].get('synthesizer', 'N/A')}\"\"\"\n",
        "        \n",
        "        return result[\"response\"], config_text\n",
        "    \n",
        "    with gr.Blocks(title=\"Advanced RAG Assistant\") as interface:\n",
        "        # Title and description\n",
        "        gr.Markdown(\"# ü§ñ Advanced RAG Assistant\")\n",
        "        gr.Markdown(\"Configure all RAG parameters for optimal performance and experiment with different settings!\")\n",
        "        \n",
        "        # Database initialization\n",
        "        init_btn = gr.Button(\"üîÑ Initialize Vector Database\", variant=\"primary\")\n",
        "        status_output = gr.Textbox(label=\"Database Status\", lines=2, interactive=False)\n",
        "        \n",
        "        # Main layout with columns\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ‚öôÔ∏è RAG Configuration\")\n",
        "                \n",
        "                # Model selection\n",
        "                model_dropdown = gr.Dropdown(\n",
        "                    choices=[\"gpt-4o\", \"gpt-4o-mini\"],\n",
        "                    value=\"gpt-4o-mini\",\n",
        "                    label=\"LLM Model\"\n",
        "                )\n",
        "                \n",
        "                # Temperature control\n",
        "                temperature_slider = gr.Slider(\n",
        "                    minimum=0.0, maximum=1.0, step=0.1, value=0.1,\n",
        "                    label=\"Temperature (0=deterministic, 1=creative)\"\n",
        "                )\n",
        "                \n",
        "                # Chunking parameters\n",
        "                chunk_size_input = gr.Number(\n",
        "                    value=512, minimum=128, maximum=2048,\n",
        "                    label=\"Chunk Size\"\n",
        "                )\n",
        "                \n",
        "                chunk_overlap_input = gr.Number(\n",
        "                    value=50, minimum=0, maximum=200,\n",
        "                    label=\"Chunk Overlap\"\n",
        "                )\n",
        "                \n",
        "                # Retrieval parameters\n",
        "                similarity_topk_slider = gr.Slider(\n",
        "                    minimum=1, maximum=20, step=1, value=5,\n",
        "                    label=\"Similarity Top-K (documents to retrieve)\"\n",
        "                )\n",
        "                \n",
        "                # Postprocessor selection\n",
        "                postprocessor_checkbox = gr.CheckboxGroup(\n",
        "                    choices=[\"SimilarityPostprocessor\"],\n",
        "                    value=[\"SimilarityPostprocessor\"],\n",
        "                    label=\"Node Postprocessors\"\n",
        "                )\n",
        "                \n",
        "                # Similarity filtering\n",
        "                similarity_cutoff_slider = gr.Slider(\n",
        "                    minimum=0.0, maximum=1.0, step=0.1, value=0.3,\n",
        "                    label=\"Similarity Cutoff (0=permissive, 1=strict)\"\n",
        "                )\n",
        "                \n",
        "                # Response synthesizer\n",
        "                synthesizer_dropdown = gr.Dropdown(\n",
        "                    choices=[\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"],\n",
        "                    value=\"TreeSummarize\",\n",
        "                    label=\"Response Synthesizer\"\n",
        "                )\n",
        "            \n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üí¨ Query Interface\")\n",
        "                \n",
        "                # Query input\n",
        "                query_input = gr.Textbox(\n",
        "                    label=\"Ask a question\",\n",
        "                    placeholder=\"Enter your question about the documents...\",\n",
        "                    lines=3\n",
        "                )\n",
        "                \n",
        "                # Submit button\n",
        "                submit_btn = gr.Button(\"üöÄ Ask Question\", variant=\"primary\")\n",
        "                \n",
        "                # Response output\n",
        "                response_output = gr.Textbox(\n",
        "                    label=\"AI Response\",\n",
        "                    lines=12,\n",
        "                    interactive=False\n",
        "                )\n",
        "                \n",
        "                # Configuration display\n",
        "                config_display = gr.Textbox(\n",
        "                    label=\"Configuration Used\",\n",
        "                    lines=8,\n",
        "                    interactive=False\n",
        "                )\n",
        "        \n",
        "        # Connect functions to components\n",
        "        init_btn.click(initialize_db, outputs=[status_output])\n",
        "        \n",
        "        submit_btn.click(\n",
        "            handle_advanced_query,\n",
        "            inputs=[\n",
        "                query_input, model_dropdown, temperature_slider,\n",
        "                chunk_size_input, chunk_overlap_input, similarity_topk_slider,\n",
        "                postprocessor_checkbox, similarity_cutoff_slider, synthesizer_dropdown\n",
        "            ],\n",
        "            outputs=[response_output, config_display]\n",
        "        )\n",
        "    \n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "advanced_interface = create_advanced_rag_interface()\n",
        "print(\"‚úÖ Advanced RAG interface created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéâ Launching Advanced RAG Assistant...\n",
            "üîó Professional interface with full parameter control!\n",
            "‚ö†Ô∏è  Make sure your OPENROUTER_API_KEY environment variable is set!\n",
            "* Running on local URL:  http://127.0.0.1:7866\n",
            "* Running on public URL: https://c5d16b73bdfc55272d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://c5d16b73bdfc55272d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:00<00:00, 198.07it/s]\n",
            "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:03<00:00, 25.57it/s]\n",
            "2025-09-21 09:38:46,583 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
            "2025-09-21 09:38:50,950 - INFO - 1 prompt is loaded, with the key: query\n",
            "2025-09-21 09:38:52,582 - INFO - query_type :, vector\n",
            "2025-09-21 09:38:54,751 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "# Launch the advanced interface\n",
        "print(\"üéâ Launching Advanced RAG Assistant...\")\n",
        "print(\"üîó Professional interface with full parameter control!\")\n",
        "print(\"‚ö†Ô∏è  Make sure your OPENROUTER_API_KEY environment variable is set!\")\n",
        "\n",
        "# Launch with sharing enabled\n",
        "advanced_interface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
