{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:19,687 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OPENROUTER_API_KEY found - ready for future LLM operations!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:25,726 - INFO - 1 prompt is loaded, with the key: query\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LlamaIndex configured with local embeddings\n",
            "   Using BAAI/bge-small-en-v1.5 for document embeddings\n"
          ]
        }
      ],
      "source": [
        "# Configure LlamaIndex Settings (Using OpenRouter - No OpenAI API Key needed)\n",
        "def setup_llamaindex_settings():\n",
        "    \"\"\"\n",
        "    Configure LlamaIndex with local embeddings and OpenRouter for LLM.\n",
        "    This solution uses local embeddings to avoid API costs.\n",
        "    \"\"\"\n",
        "    # Check for OpenRouter API key (for future use, not needed for this basic assignment)\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"ℹ️  OPENROUTER_API_KEY not found - that's OK for this assignment!\")\n",
        "        print(\"   This assignment only uses local embeddings for vector operations.\")\n",
        "    else:\n",
        "        print(\"✅ OPENROUTER_API_KEY found - ready for future LLM operations!\")\n",
        "    \n",
        "    # Configure local embeddings (no API key required)\n",
        "    Settings.embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    print(\"✅ LlamaIndex configured with local embeddings\")\n",
        "    print(\"   Using BAAI/bge-small-en-v1.5 for document embeddings\")\n",
        "\n",
        "# Setup the configuration\n",
        "setup_llamaindex_settings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 42 documents from ../data\n",
            "   Using local embeddings - no OpenAI API key required!\n"
          ]
        }
      ],
      "source": [
        "def load_documents_from_folder(folder_path: str):\n",
        "    \"\"\"\n",
        "    Load documents from a folder using SimpleDirectoryReader.\n",
        "    \n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing documents\n",
        "        \n",
        "    Returns:\n",
        "        List of documents loaded from the folder\n",
        "    \"\"\"\n",
        "    # Create SimpleDirectoryReader instance with recursive search\n",
        "    reader = SimpleDirectoryReader(\n",
        "        input_dir=folder_path,\n",
        "        recursive=True\n",
        "    )\n",
        "    \n",
        "    # Load and return documents\n",
        "    documents = reader.load_data()\n",
        "    return documents\n",
        "\n",
        "# Test the function\n",
        "test_folder = \"../data\"\n",
        "documents = load_documents_from_folder(test_folder)\n",
        "print(f\"✅ Loaded {len(documents)} documents from {test_folder}\")\n",
        "print(f\"   Using local embeddings - no OpenAI API key required!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:35,114 - WARNING - Table documents doesn't exist yet. Please add some data to create it.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Vector store created successfully\n"
          ]
        }
      ],
      "source": [
        "def create_vector_store(db_path: str = \"./vectordb\", table_name: str = \"documents\"):\n",
        "    \"\"\"\n",
        "    Create a LanceDB vector store for storing document embeddings.\n",
        "    \n",
        "    Args:\n",
        "        db_path (str): Path where the vector database will be stored\n",
        "        table_name (str): Name of the table in the vector database\n",
        "        \n",
        "    Returns:\n",
        "        LanceDBVectorStore: Configured vector store\n",
        "    \"\"\"\n",
        "    # Create the directory if it doesn't exist\n",
        "    Path(db_path).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Create vector store\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=db_path,\n",
        "        table_name=table_name\n",
        "    )\n",
        "    return vector_store\n",
        "\n",
        "# Test the function\n",
        "vector_store = create_vector_store(\"./assignment_vectordb\")\n",
        "print(f\"✅ Vector store created successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing nodes: 100%|██████████| 42/42 [00:00<00:00, 144.00it/s]\n",
            "Generating embeddings: 100%|██████████| 55/55 [00:02<00:00, 26.68it/s]\n",
            "2025-09-21 09:30:37,613 - INFO - Create new table documents adding data.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Vector index created successfully with 42 documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[90m[\u001b[0m2025-09-21T04:00:37Z \u001b[33mWARN \u001b[0m lance::dataset::write::insert\u001b[90m]\u001b[0m No existing dataset at /Users/ishandutta/Documents/code/ai-accelerator/Day_6/session_2/solutions/assignment_vectordb/documents.lance, it will be created\n"
          ]
        }
      ],
      "source": [
        "def create_vector_index(documents: List, vector_store):\n",
        "    \"\"\"\n",
        "    Create a vector index from documents using the provided vector store.\n",
        "    \n",
        "    Args:\n",
        "        documents: List of documents to index\n",
        "        vector_store: LanceDB vector store to use for storage\n",
        "        \n",
        "    Returns:\n",
        "        VectorStoreIndex: The created vector index\n",
        "    \"\"\"\n",
        "    # Create storage context with vector store\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "    \n",
        "    # Create index from documents\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents, \n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "    return index\n",
        "\n",
        "# Test the function\n",
        "index = create_vector_index(documents, vector_store)\n",
        "print(f\"✅ Vector index created successfully with {len(documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:37,731 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found 2 results for query: 'What are AI agents?'\n",
            "   Result 1: THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES\n",
            "FOR REASONING , PLANNING , AND TOOL CALLING : A S U...\n",
            "   Score: 0.6240\n",
            "   Result 2: task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m...\n",
            "   Score: 0.6195\n"
          ]
        }
      ],
      "source": [
        "def search_documents(index, query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Search for relevant documents using the vector index.\n",
        "    \n",
        "    Args:\n",
        "        index: Vector index to search\n",
        "        query (str): Search query\n",
        "        top_k (int): Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        List of retrieved document nodes\n",
        "    \"\"\"\n",
        "    # Create retriever from index\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "    \n",
        "    # Retrieve documents for the query\n",
        "    results = retriever.retrieve(query)\n",
        "    return results\n",
        "\n",
        "# Test the function\n",
        "test_query = \"What are AI agents?\"\n",
        "results = search_documents(index, test_query, top_k=2)\n",
        "print(f\"✅ Found {len(results)} results for query: '{test_query}'\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"   Result {i}: {result.text[:100]}...\")\n",
        "    print(f\"   Score: {result.score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:37,812 - INFO - query_type :, vector\n",
            "2025-09-21 09:30:37,847 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Complete Vector Database Pipeline Demonstration\n",
            "============================================================\n",
            "\n",
            "🔎 Query: 'What are AI agents?'\n",
            "   Result 1: THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES\n",
            "FOR REASONING , PLANNING , AND TOOL CALLING : A S U...\n",
            "   Score: 0.6240\n",
            "   Result 2: task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m...\n",
            "   Score: 0.6195\n",
            "\n",
            "🔎 Query: 'How to evaluate agent performance?'\n",
            "   Result 1: steps, but the answers are limited to Yes/No responses [7]. As the industry continues to pivot towar...\n",
            "   Score: 0.6757\n",
            "   Result 2: system prompt for each agent can minimize excess chatter by prompting the agents not to engage in un...\n",
            "   Score: 0.6320\n",
            "\n",
            "🔎 Query: 'Italian recipes and cooking'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:37,880 - INFO - query_type :, vector\n",
            "2025-09-21 09:30:37,902 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Result 1: # 🍝 Classic Spaghetti Carbonara Recipe\n",
            "\n",
            "## Ingredients\n",
            "- 400g spaghetti pasta\n",
            "- 4 large egg yolks\n",
            "- ...\n",
            "   Score: 0.6167\n",
            "   Result 2: Spaghetti Carbonara, Italian, 20, Easy, Pasta, 450\n",
            "Margherita Pizza, Italian, 45, Medium, Tomato, 32...\n",
            "   Score: 0.6134\n",
            "\n",
            "🔎 Query: 'Financial analysis and investment'\n",
            "   Result 1: Stock, AAPL, Apple Inc, 10000, 12500, 25.0, Medium\n",
            "Stock, GOOGL, Alphabet Inc, 8000, 9200, 15.0, Med...\n",
            "   Score: 0.5600\n",
            "   Result 2: A Comprehensive Survey of AI Agent Frameworks\n",
            "and Their Applications in Financial Services\n",
            "Satyadhar...\n",
            "   Score: 0.5580\n",
            "\n",
            "============================================================\n",
            "🎉 Solution completed successfully!\n",
            "   ✅ Documents loaded from folder\n",
            "   ✅ Vector store created with LanceDB\n",
            "   ✅ Vector index built from documents\n",
            "   ✅ Semantic search implemented and tested\n"
          ]
        }
      ],
      "source": [
        "# Demonstration: Complete pipeline with multiple queries\n",
        "print(\"🚀 Complete Vector Database Pipeline Demonstration\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test multiple search queries\n",
        "search_queries = [\n",
        "    \"What are AI agents?\",\n",
        "    \"How to evaluate agent performance?\", \n",
        "    \"Italian recipes and cooking\",\n",
        "    \"Financial analysis and investment\"\n",
        "]\n",
        "\n",
        "for query in search_queries:\n",
        "    print(f\"\\n🔎 Query: '{query}'\")\n",
        "    results = search_documents(index, query, top_k=2)\n",
        "    \n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"   Result {i}: {result.text[:100]}...\")\n",
        "        print(f\"   Score: {result.score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🎉 Solution completed successfully!\")\n",
        "print(\"   ✅ Documents loaded from folder\")\n",
        "print(\"   ✅ Vector store created with LanceDB\")\n",
        "print(\"   ✅ Vector index built from documents\") \n",
        "print(\"   ✅ Semantic search implemented and tested\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
