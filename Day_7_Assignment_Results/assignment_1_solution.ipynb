{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:19,687 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OPENROUTER_API_KEY found - ready for future LLM operations!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:25,726 - INFO - 1 prompt is loaded, with the key: query\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LlamaIndex configured with local embeddings\n",
            "   Using BAAI/bge-small-en-v1.5 for document embeddings\n"
          ]
        }
      ],
      "source": [
        "# Configure LlamaIndex Settings (Using OpenRouter - No OpenAI API Key needed)\n",
        "def setup_llamaindex_settings():\n",
        "    \"\"\"\n",
        "    Configure LlamaIndex with local embeddings and OpenRouter for LLM.\n",
        "    This solution uses local embeddings to avoid API costs.\n",
        "    \"\"\"\n",
        "    # Check for OpenRouter API key (for future use, not needed for this basic assignment)\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"‚ÑπÔ∏è  OPENROUTER_API_KEY not found - that's OK for this assignment!\")\n",
        "        print(\"   This assignment only uses local embeddings for vector operations.\")\n",
        "    else:\n",
        "        print(\"‚úÖ OPENROUTER_API_KEY found - ready for future LLM operations!\")\n",
        "    \n",
        "    # Configure local embeddings (no API key required)\n",
        "    Settings.embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ LlamaIndex configured with local embeddings\")\n",
        "    print(\"   Using BAAI/bge-small-en-v1.5 for document embeddings\")\n",
        "\n",
        "# Setup the configuration\n",
        "setup_llamaindex_settings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/Users/ishandutta/miniconda3/envs/accelerator/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 42 documents from ../data\n",
            "   Using local embeddings - no OpenAI API key required!\n"
          ]
        }
      ],
      "source": [
        "def load_documents_from_folder(folder_path: str):\n",
        "    \"\"\"\n",
        "    Load documents from a folder using SimpleDirectoryReader.\n",
        "    \n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing documents\n",
        "        \n",
        "    Returns:\n",
        "        List of documents loaded from the folder\n",
        "    \"\"\"\n",
        "    # Create SimpleDirectoryReader instance with recursive search\n",
        "    reader = SimpleDirectoryReader(\n",
        "        input_dir=folder_path,\n",
        "        recursive=True\n",
        "    )\n",
        "    \n",
        "    # Load and return documents\n",
        "    documents = reader.load_data()\n",
        "    return documents\n",
        "\n",
        "# Test the function\n",
        "test_folder = \"../data\"\n",
        "documents = load_documents_from_folder(test_folder)\n",
        "print(f\"‚úÖ Loaded {len(documents)} documents from {test_folder}\")\n",
        "print(f\"   Using local embeddings - no OpenAI API key required!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:35,114 - WARNING - Table documents doesn't exist yet. Please add some data to create it.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Vector store created successfully\n"
          ]
        }
      ],
      "source": [
        "def create_vector_store(db_path: str = \"./vectordb\", table_name: str = \"documents\"):\n",
        "    \"\"\"\n",
        "    Create a LanceDB vector store for storing document embeddings.\n",
        "    \n",
        "    Args:\n",
        "        db_path (str): Path where the vector database will be stored\n",
        "        table_name (str): Name of the table in the vector database\n",
        "        \n",
        "    Returns:\n",
        "        LanceDBVectorStore: Configured vector store\n",
        "    \"\"\"\n",
        "    # Create the directory if it doesn't exist\n",
        "    Path(db_path).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Create vector store\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=db_path,\n",
        "        table_name=table_name\n",
        "    )\n",
        "    return vector_store\n",
        "\n",
        "# Test the function\n",
        "vector_store = create_vector_store(\"./assignment_vectordb\")\n",
        "print(f\"‚úÖ Vector store created successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:00<00:00, 144.00it/s]\n",
            "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:02<00:00, 26.68it/s]\n",
            "2025-09-21 09:30:37,613 - INFO - Create new table documents adding data.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Vector index created successfully with 42 documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[90m[\u001b[0m2025-09-21T04:00:37Z \u001b[33mWARN \u001b[0m lance::dataset::write::insert\u001b[90m]\u001b[0m No existing dataset at /Users/ishandutta/Documents/code/ai-accelerator/Day_6/session_2/solutions/assignment_vectordb/documents.lance, it will be created\n"
          ]
        }
      ],
      "source": [
        "def create_vector_index(documents: List, vector_store):\n",
        "    \"\"\"\n",
        "    Create a vector index from documents using the provided vector store.\n",
        "    \n",
        "    Args:\n",
        "        documents: List of documents to index\n",
        "        vector_store: LanceDB vector store to use for storage\n",
        "        \n",
        "    Returns:\n",
        "        VectorStoreIndex: The created vector index\n",
        "    \"\"\"\n",
        "    # Create storage context with vector store\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "    \n",
        "    # Create index from documents\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents, \n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "    return index\n",
        "\n",
        "# Test the function\n",
        "index = create_vector_index(documents, vector_store)\n",
        "print(f\"‚úÖ Vector index created successfully with {len(documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:37,731 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 2 results for query: 'What are AI agents?'\n",
            "   Result 1: THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES\n",
            "FOR REASONING , PLANNING , AND TOOL CALLING : A S U...\n",
            "   Score: 0.6240\n",
            "   Result 2: task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m...\n",
            "   Score: 0.6195\n"
          ]
        }
      ],
      "source": [
        "def search_documents(index, query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Search for relevant documents using the vector index.\n",
        "    \n",
        "    Args:\n",
        "        index: Vector index to search\n",
        "        query (str): Search query\n",
        "        top_k (int): Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        List of retrieved document nodes\n",
        "    \"\"\"\n",
        "    # Create retriever from index\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "    \n",
        "    # Retrieve documents for the query\n",
        "    results = retriever.retrieve(query)\n",
        "    return results\n",
        "\n",
        "# Test the function\n",
        "test_query = \"What are AI agents?\"\n",
        "results = search_documents(index, test_query, top_k=2)\n",
        "print(f\"‚úÖ Found {len(results)} results for query: '{test_query}'\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"   Result {i}: {result.text[:100]}...\")\n",
        "    print(f\"   Score: {result.score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:37,812 - INFO - query_type :, vector\n",
            "2025-09-21 09:30:37,847 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Complete Vector Database Pipeline Demonstration\n",
            "============================================================\n",
            "\n",
            "üîé Query: 'What are AI agents?'\n",
            "   Result 1: THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES\n",
            "FOR REASONING , PLANNING , AND TOOL CALLING : A S U...\n",
            "   Score: 0.6240\n",
            "   Result 2: task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m...\n",
            "   Score: 0.6195\n",
            "\n",
            "üîé Query: 'How to evaluate agent performance?'\n",
            "   Result 1: steps, but the answers are limited to Yes/No responses [7]. As the industry continues to pivot towar...\n",
            "   Score: 0.6757\n",
            "   Result 2: system prompt for each agent can minimize excess chatter by prompting the agents not to engage in un...\n",
            "   Score: 0.6320\n",
            "\n",
            "üîé Query: 'Italian recipes and cooking'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-21 09:30:37,880 - INFO - query_type :, vector\n",
            "2025-09-21 09:30:37,902 - INFO - query_type :, vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Result 1: # üçù Classic Spaghetti Carbonara Recipe\n",
            "\n",
            "## Ingredients\n",
            "- 400g spaghetti pasta\n",
            "- 4 large egg yolks\n",
            "- ...\n",
            "   Score: 0.6167\n",
            "   Result 2: Spaghetti Carbonara, Italian, 20, Easy, Pasta, 450\n",
            "Margherita Pizza, Italian, 45, Medium, Tomato, 32...\n",
            "   Score: 0.6134\n",
            "\n",
            "üîé Query: 'Financial analysis and investment'\n",
            "   Result 1: Stock, AAPL, Apple Inc, 10000, 12500, 25.0, Medium\n",
            "Stock, GOOGL, Alphabet Inc, 8000, 9200, 15.0, Med...\n",
            "   Score: 0.5600\n",
            "   Result 2: A Comprehensive Survey of AI Agent Frameworks\n",
            "and Their Applications in Financial Services\n",
            "Satyadhar...\n",
            "   Score: 0.5580\n",
            "\n",
            "============================================================\n",
            "üéâ Solution completed successfully!\n",
            "   ‚úÖ Documents loaded from folder\n",
            "   ‚úÖ Vector store created with LanceDB\n",
            "   ‚úÖ Vector index built from documents\n",
            "   ‚úÖ Semantic search implemented and tested\n"
          ]
        }
      ],
      "source": [
        "# Demonstration: Complete pipeline with multiple queries\n",
        "print(\"üöÄ Complete Vector Database Pipeline Demonstration\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test multiple search queries\n",
        "search_queries = [\n",
        "    \"What are AI agents?\",\n",
        "    \"How to evaluate agent performance?\", \n",
        "    \"Italian recipes and cooking\",\n",
        "    \"Financial analysis and investment\"\n",
        "]\n",
        "\n",
        "for query in search_queries:\n",
        "    print(f\"\\nüîé Query: '{query}'\")\n",
        "    results = search_documents(index, query, top_k=2)\n",
        "    \n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"   Result {i}: {result.text[:100]}...\")\n",
        "        print(f\"   Score: {result.score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ Solution completed successfully!\")\n",
        "print(\"   ‚úÖ Documents loaded from folder\")\n",
        "print(\"   ‚úÖ Vector store created with LanceDB\")\n",
        "print(\"   ‚úÖ Vector index built from documents\") \n",
        "print(\"   ‚úÖ Semantic search implemented and tested\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
