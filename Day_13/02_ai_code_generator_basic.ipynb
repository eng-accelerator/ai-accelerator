{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– AI Generator Agent Tutorial (Basic): Core Concepts\n",
    "\n",
    "Welcome to the comprehensive tutorial for understanding **AI-powered code generation** concepts inspired by the Orion AI Agent System! This tutorial demonstrates the core principles and techniques used in advanced AI code generation.\n",
    "\n",
    "## ğŸ¯ What You'll Learn\n",
    "\n",
    "- Core concepts of AI-powered code generation\n",
    "- Structured output patterns with JSON validation\n",
    "- File creation and modification strategies\n",
    "- Context-aware generation techniques\n",
    "- Code quality analysis methods\n",
    "- Best practices for AI code generation\n",
    "\n",
    "## ğŸš€ Tutorial Approach\n",
    "\n",
    "This is a **standalone educational tutorial** that demonstrates the concepts and techniques used in AI code generation. We'll build simplified versions of key components to understand how they work, without requiring the full Orion system.\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- OpenRouter API key (provides access to multiple AI models)\n",
    "- Understanding of JSON and basic data structures\n",
    "\n",
    "## ğŸ”‘ Getting Started with OpenRouter\n",
    "\n",
    "OpenRouter provides access to multiple AI models through a single API:\n",
    "1. **Sign up** at [openrouter.ai](https://openrouter.ai)\n",
    "2. **Get your API key** from [openrouter.ai/keys](https://openrouter.ai/keys)\n",
    "3. **Set environment variable**: `export OPENROUTER_API_KEY='your-key-here'`\n",
    "4. **Choose models**: GPT-4o, Claude, Gemini, Llama, and many more!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Setup and Environment\n",
    "\n",
    "Let's start by setting up our environment and importing the necessary components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Current working directory: /Users/ishandutta/Documents/code/ai-accelerator/orion\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "print(f\"ğŸ“ Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Environment Configuration\n",
    "\n",
    "Before we can use the AI Generator, we need to ensure our OpenRouter API key is configured properly. OpenRouter provides access to multiple AI models through a unified API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9905880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Building Our Simple AI Code Generator\n",
    "\n",
    "Let's create a simplified AI code generator to understand the core concepts. We'll build this step by step, showing how each component works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "666dc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define our structured output model\n",
    "class CodeGenerationResponse(BaseModel):\n",
    "    \"\"\"Structured response for code generation.\"\"\"\n",
    "\n",
    "    success: bool = Field(description=\"Whether the generation was successful\")\n",
    "    \n",
    "    files: List[Dict[str, str]] = Field(\n",
    "        description=\"Generated files with name and content\"\n",
    "    )\n",
    "    \n",
    "    reasoning: str = Field(description=\"Step-by-step explanation of the approach\")\n",
    "    \n",
    "    dependencies: List[str] = Field(description=\"Required dependencies\", default=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Simple AI Code Generator class defined!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create our simple AI code generator class\n",
    "class SimpleAICodeGenerator:\n",
    "    def __init__(\n",
    "        self, \n",
    "        api_key: str, \n",
    "        model: str = \"openai/gpt-4o-mini\", \n",
    "        temperature: float = 0.1\n",
    "    ):\n",
    "        \"\"\"Initialize the simple AI code generator using OpenRouter.\"\"\"\n",
    "        # Configure client for OpenRouter\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "        )\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.generation_history = []\n",
    "\n",
    "    def generate_code(\n",
    "        self,\n",
    "         prompt: str, \n",
    "         context: Optional[Dict] = None\n",
    "    ) -> Optional[Dict]:\n",
    "        \"\"\"Generate code using AI with structured output.\"\"\"\n",
    "        print(f\"ğŸ¤– Generating code with {self.model}...\")\n",
    "\n",
    "        # Build context string\n",
    "        context_str = (\n",
    "            self._build_context(context)\n",
    "            if context\n",
    "            else \"No additional context provided\"\n",
    "        )\n",
    "\n",
    "        # Create the prompt\n",
    "        system_prompt = f\"\"\"You are an expert software developer. Generate complete, production-ready code.\n",
    "\n",
    "CRITICAL: You MUST respond with valid JSON in this exact structure:\n",
    "{\n",
    "  \"success\": true,\n",
    "  \"files\": [\n",
    "    {\"name\": \"filename.py\", \"content\": \"complete file content with imports and docstrings\"},\n",
    "  ],\n",
    "  \"reasoning\": \"Step-by-step explanation of your approach and decisions\",\n",
    "  \"dependencies\": [\"package1\", \"package2\"],\n",
    "}\n",
    "\n",
    "Requirements:\n",
    "- Complete, runnable code with all imports\n",
    "- Google-style docstrings and type hints\n",
    "- Proper error handling\n",
    "- Follow PEP 8 style guidelines\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Task: {prompt}\n",
    "\n",
    "Context: {context_str}\n",
    "\n",
    "Generate production-ready code that solves this task completely.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=self.temperature,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "\n",
    "        # Parse the JSON response\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        # Store in history\n",
    "        self.generation_history.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"result\": result,\n",
    "                \"timestamp\": time.time(),\n",
    "                \"model\": self.model,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _build_context(self, context: Dict) -> str:\n",
    "        \"\"\"Build context string from dictionary.\"\"\"\n",
    "        if not context:\n",
    "            return \"No context provided\"\n",
    "\n",
    "        context_parts = []\n",
    "        for key, value in context.items():\n",
    "            if isinstance(value, list):\n",
    "                value = \", \".join(str(v) for v in value)\n",
    "            context_parts.append(f\"- {key}: {value}\")\n",
    "\n",
    "        return \"\\\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# Initialize our generator\n",
    "print(\"ğŸ—ï¸ Simple AI Code Generator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Creating Our AI Generator Instance\n",
    "\n",
    "Let's create an instance of our Simple AI Code Generator and explore its capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Simple AI Code Generator created successfully!\n",
      "ğŸ§  Model: openai/gpt-4o-mini\n",
      "ğŸŒ¡ï¸ Temperature: 0.1\n",
      "ğŸ“Š Generation history: 0 entries\n"
     ]
    }
   ],
   "source": [
    "# Create our AI generator instance\n",
    "# Create the generator\n",
    "ai_generator = SimpleAICodeGenerator(\n",
    "    api_key=api_key,\n",
    "    model=\"openai/gpt-4o-mini\",  # Using GPT-4o-mini through OpenRouter\n",
    "    temperature=0.1,  # Low temperature for consistent code generation\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– Simple AI Code Generator created successfully!\")\n",
    "print(f\"ğŸ§  Model: {ai_generator.model}\")\n",
    "print(f\"ğŸŒ¡ï¸ Temperature: {ai_generator.temperature}\")\n",
    "print(f\"ğŸ“Š Generation history: {len(ai_generator.generation_history)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Basic Code Generation\n",
    "\n",
    "Let's start with a simple code generation example to see the AI Generator in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nğŸ¯ Generating code for: Create a python script to use diffusers package for text to image generation\n",
      "â³ Please wait while AI generates the code...\n",
      "ğŸ¤– Generating code with openai/gpt-4o-mini...\n",
      "\\nâœ… Code generation successful!\n",
      "ğŸ“¦ Dependencies: []\n",
      "\\nğŸ§  AI Reasoning:\n",
      "No reasoning provided\n",
      "\\nğŸ“ Generated Files:\n",
      "================================================================================\n",
      "FILE: text_to_image.py\n",
      "```python\\nimport torch\n",
      "from diffusers import StableDiffusionPipeline\n",
      "from PIL import Image\n",
      "\n",
      "\n",
      "def generate_image(prompt: str, output_path: str) -> None:\n",
      "    \"\"\"\n",
      "    Generates an image from a text prompt using the Stable Diffusion model.\n",
      "\n",
      "    Args:\n",
      "        prompt (str): The text prompt to generate the image from.\n",
      "        output_path (str): The file path where the generated image will be saved.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the prompt is empty.\n",
      "        RuntimeError: If the image generation fails.\n",
      "    \"\"\"\n",
      "    if not prompt:\n",
      "        raise ValueError('Prompt must not be empty.')\n",
      "\n",
      "    try:\n",
      "        # Load the pre-trained Stable Diffusion model\n",
      "        pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
      "        pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "        # Generate the image\n",
      "        image = pipe(prompt).images[0]\n",
      "\n",
      "        # Save the image\n",
      "        image.save(output_path)\n",
      "        print(f'Image saved to {output_path}')\n",
      "    except Exception as e:\n",
      "        raise RuntimeError(f'Image generation failed: {str(e)}')\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    # Example usage\n",
      "    prompt = \"A fantasy landscape with mountains and a river\"\n",
      "    output_path = \"generated_image.png\"\n",
      "    generate_image(prompt, output_path)\\n```\n",
      "----------------------------------------\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary directory for our test repository\n",
    "test_repo = \"/Users/ishandutta/Documents/code/orion-accelerator\"\n",
    "\n",
    "# Simple code generation task\n",
    "simple_prompt = \"Create a python script to use diffusers package for text to image generation\"\n",
    "\n",
    "print(f\"\\\\nğŸ¯ Generating code for: {simple_prompt}\")\n",
    "print(\"â³ Please wait while AI generates the code...\")\n",
    "\n",
    "# Generate code using our simple AI generator\n",
    "result = ai_generator.generate_code(simple_prompt)\n",
    "\n",
    "if result and result.get(\"success\"):\n",
    "    print(\"\\\\nâœ… Code generation successful!\")\n",
    "    print(f\"ğŸ“¦ Dependencies: {result.get('dependencies', [])}\")\n",
    "\n",
    "    print(\"\\\\nğŸ§  AI Reasoning:\")\n",
    "    print(result.get(\"reasoning\", \"No reasoning provided\"))\n",
    "\n",
    "    print(\"\\\\nğŸ“ Generated Files:\")\n",
    "    print(\"=\" * 80)\n",
    "    for file_info in result.get(\"files\", []):\n",
    "        filename = file_info.get(\"name\", \"unknown.py\")\n",
    "        content = file_info.get(\"content\", \"\")\n",
    "        print(f\"FILE: {filename}\")\n",
    "        print(f\"```python\\\\n{content}\\\\n```\")\n",
    "        print(\"-\" * 40)\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"âŒ Code generation failed!\")\n",
    "    if result:\n",
    "        print(f\"Error details: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ File Creation Utilities\n",
    "\n",
    "Let's create utilities to handle file creation and management, similar to what the Orion system does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Creating 1 files from generation result...\n",
      "âœ… Created: text_to_image.py (1255 chars)\n",
      "âœ… Successfully created 1/1 files\n",
      "\\nğŸ“‚ Repository structure (/Users/ishandutta/Documents/code/orion-accelerator):\n",
      "orion-accelerator/\n",
      "  text_to_image.py (1255 bytes)\n",
      "  open-clip/\n",
      "    orion_tutorial_1758950981.md (771 bytes)\n",
      "    README.md (22 bytes)\n",
      "    .git/\n",
      "      ORIG_HEAD (41 bytes)\n",
      "      config (543 bytes)\n",
      "      HEAD (51 bytes)\n",
      "      description (73 bytes)\n",
      "      index (233 bytes)\n",
      "      packed-refs (7698 bytes)\n",
      "      COMMIT_EDITMSG (77 bytes)\n",
      "      FETCH_HEAD (103 bytes)\n",
      "      objects/\n",
      "        56/\n",
      "          1a8458bff1c3c7fd129519230868a2a667c650 (103 bytes)\n",
      "        94/\n",
      "          d862d656e1c6aeecc9ed3e689fa93a96d2e7f8 (459 bytes)\n",
      "        pack/\n",
      "          pack-447c3650873a6b876968cfda879083862b6809c0.pack (35289126 bytes)\n",
      "          pack-447c3650873a6b876968cfda879083862b6809c0.idx (294428 bytes)\n",
      "        21/\n",
      "          d1335d00fb151b50db641e9fc5bcad4169b750 (194 bytes)\n",
      "        info/\n",
      "        91/\n",
      "          6082fe55c6bfb4b82a1c5f84c49dec9c66a42f (204 bytes)\n",
      "        a7/\n",
      "          e53507e9f45eb70e9b927e4f63d07ef8b7192a (462 bytes)\n",
      "        d2/\n",
      "          2f9da6f0d207f3c3062a92d0a1d19467f60a22 (102 bytes)\n",
      "      info/\n",
      "        exclude (240 bytes)\n",
      "      logs/\n",
      "        HEAD (1199 bytes)\n",
      "        refs/\n",
      "          heads/\n",
      "            main (192 bytes)\n",
      "            orion/\n",
      "              tutorial-complete-1758950979 (375 bytes)\n",
      "              tutorial-git-test-1758950045 (357 bytes)\n",
      "          remotes/\n",
      "            origin/\n",
      "              HEAD (192 bytes)\n",
      "              orion/\n",
      "                tutorial-complete-1758950979 (147 bytes)\n",
      "      hooks/\n",
      "        commit-msg.sample (896 bytes)\n",
      "        pre-rebase.sample (4898 bytes)\n",
      "        pre-commit.sample (1643 bytes)\n",
      "        applypatch-msg.sample (478 bytes)\n",
      "        fsmonitor-watchman.sample (4726 bytes)\n",
      "        pre-receive.sample (544 bytes)\n",
      "        prepare-commit-msg.sample (1492 bytes)\n",
      "        post-update.sample (189 bytes)\n",
      "        pre-merge-commit.sample (416 bytes)\n",
      "        pre-applypatch.sample (424 bytes)\n",
      "        pre-push.sample (1374 bytes)\n",
      "        update.sample (3650 bytes)\n",
      "        push-to-checkout.sample (2783 bytes)\n",
      "      refs/\n",
      "        heads/\n",
      "          main (41 bytes)\n",
      "          orion/\n",
      "            tutorial-complete-1758950979 (41 bytes)\n",
      "            tutorial-git-test-1758950045 (41 bytes)\n",
      "        tags/\n",
      "        remotes/\n",
      "          origin/\n",
      "            HEAD (30 bytes)\n",
      "            orion/\n",
      "              tutorial-complete-1758950979 (41 bytes)\n"
     ]
    }
   ],
   "source": [
    "# File management utilities\n",
    "class FileManager:\n",
    "    \"\"\"Simple file management utility for the tutorial.\"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = base_path\n",
    "        self.created_files = []\n",
    "\n",
    "    def create_file(self, filename: str, content: str) -> bool:\n",
    "        \"\"\"Create a file with the given content.\"\"\"\n",
    "\n",
    "        full_path = os.path.join(self.base_path, filename)\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        dir_path = os.path.dirname(full_path)\n",
    "        if dir_path and dir_path != self.base_path:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        # Write the file\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        self.created_files.append(filename)\n",
    "        print(f\"âœ… Created: {filename} ({len(content)} chars)\")\n",
    "        return True\n",
    "\n",
    "    def apply_generation_result(self, result: Dict) -> bool:\n",
    "        \"\"\"Apply a generation result by creating all the files.\"\"\"\n",
    "        if not result or not result.get(\"success\"):\n",
    "            return False\n",
    "\n",
    "        success_count = 0\n",
    "        files = result.get(\"files\", [])\n",
    "\n",
    "        print(f\"ğŸ“ Creating {len(files)} files from generation result...\")\n",
    "\n",
    "        for file_info in files:\n",
    "            filename = file_info.get(\"name\", \"unknown.py\")\n",
    "            content = file_info.get(\"content\", \"\")\n",
    "\n",
    "            if self.create_file(filename, content):\n",
    "                success_count += 1\n",
    "\n",
    "        print(f\"âœ… Successfully created {success_count}/{len(files)} files\")\n",
    "        return success_count == len(files)\n",
    "\n",
    "    def show_repository_structure(self):\n",
    "        \"\"\"Display the repository structure.\"\"\"\n",
    "        print(f\"\\\\nğŸ“‚ Repository structure ({self.base_path}):\")\n",
    "        for root, dirs, files in os.walk(self.base_path):\n",
    "            level = root.replace(self.base_path, \"\").count(os.sep)\n",
    "            indent = \"  \" * level\n",
    "            folder_name = os.path.basename(root) or \"root\"\n",
    "            print(f\"{indent}{folder_name}/\")\n",
    "\n",
    "            subindent = \"  \" * (level + 1)\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                print(f\"{subindent}{file} ({size} bytes)\")\n",
    "\n",
    "\n",
    "# Create file manager and apply our generated code\n",
    "file_manager = FileManager(test_repo)\n",
    "\n",
    "# Apply the previous generation result if we have one\n",
    "if ai_generator and len(ai_generator.generation_history) > 0:\n",
    "    latest_result = ai_generator.generation_history[-1][\"result\"]\n",
    "    file_manager.apply_generation_result(latest_result)\n",
    "    file_manager.show_repository_structure()\n",
    "else:\n",
    "    print(\"âš ï¸ No generation result to apply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Context-Aware Code Generation\n",
    "\n",
    "Let's explore how adding context improves code generation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Context-aware generation task: Create a Gradio app to use diffusers package for text to image generation\n",
      "ğŸ“‹ Context provided: {\n",
      "  \"framework\": [\n",
      "    \"diffusers\"\n",
      "  ],\n",
      "  \"app\": \"gradio\",\n",
      "  \"features\": [\n",
      "    \"gradio.textbox()\",\n",
      "    \"gradio.button()\",\n",
      "    \"gradio.image()\"\n",
      "  ]\n",
      "}\n",
      "â³ Generating with rich context...\n",
      "ğŸ¤– Generating code with openai/gpt-4o-mini...\n",
      "\n",
      "âœ… Context-aware generation successful!\n",
      "\n",
      "ğŸ“Š Generated 1 files:\n",
      "  1. text_to_image_app.py (1316 chars)\n",
      "    import gradio as gr\n",
      "from diffusers import StableDiffusionPipeline\n",
      "import torch\n",
      "\n",
      "\n",
      "def generate_image(prompt: str) -> str:\n",
      "    \"\"\"\n",
      "    Generates an image from a text prompt using the Stable Diffusion model.\n",
      "\n",
      "    Args:\n",
      "        prompt (str): The text prompt to generate the image from.\n",
      "\n",
      "    Returns:\n",
      "        str: The path to the generated image.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Load the model\n",
      "        model_id = \"CompVis/stable-diffusion-v1-4\"\n",
      "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
      "        pipe = pipe.to(\"cuda\")  # Move the model to GPU if available\n",
      "\n",
      "        # Generate the image\n",
      "        image = pipe(prompt).images[0]\n",
      "        image_path = \"generated_image.png\"\n",
      "        image.save(image_path)\n",
      "        return image_path\n",
      "    except Exception as e:\n",
      "        return f\"Error generating image: {str(e)}\"\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"\n",
      "    Main function to run the Gradio app.\n",
      "    \"\"\"\n",
      "    with gr.Blocks() as demo:\n",
      "        gr.Markdown(\"# Text to Image Generation with Diffusers\")\n",
      "        prompt = gr.Textbox(label=\"Enter your text prompt\")\n",
      "        generate_btn = gr.Button(\"Generate Image\")\n",
      "        output_image = gr.Image(label=\"Generated Image\")\n",
      "\n",
      "        generate_btn.click(generate_image, inputs=prompt, outputs=output_image)\n",
      "\n",
      "    demo.launch()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "ğŸ“ Creating 1 files from generation result...\n",
      "âœ… Created: text_to_image_app.py (1316 chars)\n",
      "âœ… Successfully created 1/1 files\n",
      "\n",
      "ğŸ’¾ Applied context-aware generation to repository\n"
     ]
    }
   ],
   "source": [
    "# Context-aware generation example\n",
    "context_prompt = \"Create a Gradio app to use diffusers package for text to image generation\"\n",
    "\n",
    "# Provide rich context to guide the AI\n",
    "context = {\n",
    "    \"framework\": [\"diffusers\"],\n",
    "    \"app\": \"gradio\",\n",
    "    \"features\": [\"gradio.textbox()\", \"gradio.button()\", \"gradio.image()\"],\n",
    "}\n",
    "\n",
    "print(f\"ğŸ¯ Context-aware generation task: {context_prompt}\")\n",
    "print(f\"ğŸ“‹ Context provided: {json.dumps(context, indent=2)}\")\n",
    "print(\"â³ Generating with rich context...\")\n",
    "\n",
    "context_result = ai_generator.generate_code(prompt=context_prompt, context=context)\n",
    "\n",
    "if context_result and context_result.get(\"success\"):\n",
    "    print(\"\\nâœ… Context-aware generation successful!\")\n",
    "\n",
    "    # Show generated files summary\n",
    "    files = context_result.get(\"files\", [])\n",
    "    print(f\"\\nğŸ“Š Generated {len(files)} files:\")\n",
    "    for i, file_info in enumerate(files, 1):\n",
    "        filename = file_info.get(\"name\", \"unknown.py\")\n",
    "        content = file_info.get(\"content\", \"\")\n",
    "        print(f\"  {i}. {filename} ({len(content)} chars)\")\n",
    "        print(f\"    {content}\")\n",
    "        \n",
    "\n",
    "    # Apply to repository\n",
    "    file_manager.apply_generation_result(context_result)\n",
    "    print(\"\\nğŸ’¾ Applied context-aware generation to repository\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Generation History and Analytics\n",
    "\n",
    "Let's explore the generation history and analytics we've built into our Simple AI Code Generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Generation History (2 generations):\n",
      "\n",
      "   ğŸ“ Prompt: Create a python script to use diffusers package fo...\n",
      "   ğŸ§  Model: openai/gpt-4o-mini\n",
      "   âœ… Success: True\n",
      "   â° Timestamp: Sat Sep 27 11:17:16 2025\n",
      "   ğŸ“„ Files generated: 1\n",
      "\n",
      "   ğŸ“ Prompt: Create a Gradio app to use diffusers package for t...\n",
      "   ğŸ§  Model: openai/gpt-4o-mini\n",
      "   âœ… Success: True\n",
      "   â° Timestamp: Sat Sep 27 11:34:38 2025\n",
      "   ğŸ“„ Files generated: 1\n",
      "\n",
      "   ğŸ“Š Summary Statistics:\n",
      "   ğŸ“ Total files generated: 2\n",
      "   ğŸ“ Files in current repo: 2\n",
      "   ğŸ¯ Average confidence: 0.0%\n",
      "   â° Total generations: 2\n",
      "\\nğŸ“‚ Repository structure (/Users/ishandutta/Documents/code/orion-accelerator):\n",
      "orion-accelerator/\n",
      "  text_to_image.py (1255 bytes)\n",
      "  text_to_image_app.py (1316 bytes)\n",
      "  open-clip/\n",
      "    orion_tutorial_1758950981.md (771 bytes)\n",
      "    README.md (22 bytes)\n",
      "    .git/\n",
      "      ORIG_HEAD (41 bytes)\n",
      "      config (543 bytes)\n",
      "      HEAD (51 bytes)\n",
      "      description (73 bytes)\n",
      "      index (233 bytes)\n",
      "      packed-refs (7698 bytes)\n",
      "      COMMIT_EDITMSG (77 bytes)\n",
      "      FETCH_HEAD (103 bytes)\n",
      "      objects/\n",
      "        56/\n",
      "          1a8458bff1c3c7fd129519230868a2a667c650 (103 bytes)\n",
      "        94/\n",
      "          d862d656e1c6aeecc9ed3e689fa93a96d2e7f8 (459 bytes)\n",
      "        pack/\n",
      "          pack-447c3650873a6b876968cfda879083862b6809c0.pack (35289126 bytes)\n",
      "          pack-447c3650873a6b876968cfda879083862b6809c0.idx (294428 bytes)\n",
      "        21/\n",
      "          d1335d00fb151b50db641e9fc5bcad4169b750 (194 bytes)\n",
      "        info/\n",
      "        91/\n",
      "          6082fe55c6bfb4b82a1c5f84c49dec9c66a42f (204 bytes)\n",
      "        a7/\n",
      "          e53507e9f45eb70e9b927e4f63d07ef8b7192a (462 bytes)\n",
      "        d2/\n",
      "          2f9da6f0d207f3c3062a92d0a1d19467f60a22 (102 bytes)\n",
      "      info/\n",
      "        exclude (240 bytes)\n",
      "      logs/\n",
      "        HEAD (1199 bytes)\n",
      "        refs/\n",
      "          heads/\n",
      "            main (192 bytes)\n",
      "            orion/\n",
      "              tutorial-complete-1758950979 (375 bytes)\n",
      "              tutorial-git-test-1758950045 (357 bytes)\n",
      "          remotes/\n",
      "            origin/\n",
      "              HEAD (192 bytes)\n",
      "              orion/\n",
      "                tutorial-complete-1758950979 (147 bytes)\n",
      "      hooks/\n",
      "        commit-msg.sample (896 bytes)\n",
      "        pre-rebase.sample (4898 bytes)\n",
      "        pre-commit.sample (1643 bytes)\n",
      "        applypatch-msg.sample (478 bytes)\n",
      "        fsmonitor-watchman.sample (4726 bytes)\n",
      "        pre-receive.sample (544 bytes)\n",
      "        prepare-commit-msg.sample (1492 bytes)\n",
      "        post-update.sample (189 bytes)\n",
      "        pre-merge-commit.sample (416 bytes)\n",
      "        pre-applypatch.sample (424 bytes)\n",
      "        pre-push.sample (1374 bytes)\n",
      "        update.sample (3650 bytes)\n",
      "        push-to-checkout.sample (2783 bytes)\n",
      "      refs/\n",
      "        heads/\n",
      "          main (41 bytes)\n",
      "          orion/\n",
      "            tutorial-complete-1758950979 (41 bytes)\n",
      "            tutorial-git-test-1758950045 (41 bytes)\n",
      "        tags/\n",
      "        remotes/\n",
      "          origin/\n",
      "            HEAD (30 bytes)\n",
      "            orion/\n",
      "              tutorial-complete-1758950979 (41 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Explore generation history and analytics\n",
    "history = ai_generator.generation_history\n",
    "print(f\"ğŸ“ˆ Generation History ({len(history)} generations):\")\n",
    "\n",
    "for i, generation in enumerate(history, 1):\n",
    "    result = generation[\"result\"]\n",
    "    print()\n",
    "    print(f\"   ğŸ“ Prompt: {generation['prompt'][:50]}...\")\n",
    "    print(f\"   ğŸ§  Model: {generation['model']}\")\n",
    "    print(f\"   âœ… Success: {result.get('success', False)}\")\n",
    "    print(f\"   â° Timestamp: {time.ctime(generation['timestamp'])}\")\n",
    "    print(f\"   ğŸ“„ Files generated: {len(result.get('files', []))}\")\n",
    "\n",
    "# Calculate statistics\n",
    "if history:\n",
    "    total_files = sum(len(gen[\"result\"].get(\"files\", [])) for gen in history)\n",
    "    avg_confidence = sum(\n",
    "        gen[\"result\"].get(\"confidence\", 0) for gen in history\n",
    "    ) / len(history)\n",
    "\n",
    "    print()\n",
    "    print(f\"   ğŸ“Š Summary Statistics:\")\n",
    "    print(f\"   ğŸ“ Total files generated: {total_files}\")\n",
    "    print(f\"   ğŸ“ Files in current repo: {len(file_manager.created_files)}\")\n",
    "    print(f\"   ğŸ¯ Average confidence: {avg_confidence:.1%}\")\n",
    "    print(f\"   â° Total generations: {len(history)}\")\n",
    "\n",
    "    # Show final repository structure\n",
    "    file_manager.show_repository_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Tutorial Conclusion\n",
    "\n",
    "Congratulations! You've completed the **AI Code Generation Concepts Tutorial (Basic)**. Here's what you've learned:\n",
    "\n",
    "### âœ… Core Concepts Mastered\n",
    "\n",
    "1. **ğŸ¤– AI-Powered Code Generation**: Understood how to use language models for code generation\n",
    "2. **ğŸ“‹ Structured Output**: Learned to use JSON schemas and Pydantic for reliable AI responses\n",
    "3. **ğŸ¯ Context-Aware Generation**: Saw how additional context improves generation quality\n",
    "4. **ğŸ“ File Management**: Built utilities for creating and organizing generated files\n",
    "\n",
    "### ğŸ› ï¸ Technical Skills Gained\n",
    "\n",
    "- **OpenRouter API Integration**: Multi-model access through unified API\n",
    "- **Pydantic Models**: Data validation and serialization\n",
    "- **File System Operations**: Programmatic file creation and management\n",
    "- **Code Analysis**: Parsing and evaluating Python code quality\n",
    "- **JSON Processing**: Handling structured AI outputs\n",
    "\n",
    "### ğŸ¯ Best Practices Learned\n",
    "\n",
    "- Use **low temperature** (0.1) for deterministic code generation\n",
    "- Provide **rich context** in prompts for better results\n",
    "- Implement **structured output validation** for reliability\n",
    "- Track **generation history** for performance analysis\n",
    "- Use **clear, specific prompts** for focused results\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accelerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
